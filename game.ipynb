{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pulp import LpMinimize, LpMaximize, LpProblem, LpStatus, lpSum, LpVariable, value, GLPK\n",
    "\n",
    "OBJ_EPSILON = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "class Game(object):\n",
    "    def __init__(self, config, env, random_seed=1000):\n",
    "        self.random_state = np.random.RandomState(seed=random_seed)\n",
    " \n",
    "        self.data_dir = env.data_dir\n",
    "        self.DG = env.topology.DG\n",
    "        self.traffic_file = env.traffic_file\n",
    "        self.traffic_matrices = env.traffic_matrices\n",
    "        self.traffic_matrices_dims = self.traffic_matrices.shape\n",
    "        self.tm_cnt = env.tm_cnt\n",
    "        self.num_pairs = env.num_pairs\n",
    "        self.pair_idx_to_sd = env.pair_idx_to_sd\n",
    "        self.pair_sd_to_idx = env.pair_sd_to_idx\n",
    "        self.num_nodes = env.num_nodes\n",
    "        self.num_links = env.num_links\n",
    "        self.link_idx_to_sd = env.link_idx_to_sd\n",
    "        self.link_sd_to_idx = env.link_sd_to_idx\n",
    "        self.link_capacities = env.link_capacities\n",
    "        self.link_weights = env.link_weights\n",
    "        self.shortest_paths_node = env.shortest_paths_node              # paths with node info\n",
    "        self.shortest_paths_link = env.shortest_paths_link              # paths with link info\n",
    "\n",
    "        self.get_ecmp_next_hops()\n",
    "        \n",
    "        self.model_type = config.model_type\n",
    "        \n",
    "        #for LP\n",
    "        self.lp_pairs = [p for p in range(self.num_pairs)]\n",
    "        self.lp_nodes = [n for n in range(self.num_nodes)]\n",
    "        self.links = [e for e in range(self.num_links)]\n",
    "        self.lp_links = [e for e in self.link_sd_to_idx]\n",
    "        self.pair_links = [(pr, e[0], e[1]) for pr in self.lp_pairs for e in self.lp_links]\n",
    "\n",
    "        self.load_multiplier = {}\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def generate_inputs(self, normalization=True):\n",
    "        self.normalized_traffic_matrices = np.zeros((self.valid_tm_cnt, self.traffic_matrices_dims[1], self.traffic_matrices_dims[2], self.tm_history), dtype=np.float32)   #tm state  [Valid_tms, Node, Node, History]\n",
    "        idx_offset = self.tm_history - 1\n",
    "        for tm_idx in self.tm_indexes:\n",
    "            for h in range(self.tm_history):\n",
    "                if normalization:\n",
    "                    tm_max_element = np.max(self.traffic_matrices[tm_idx-h])\n",
    "                    self.normalized_traffic_matrices[tm_idx-idx_offset,:,:,h] = self.traffic_matrices[tm_idx-h] / tm_max_element        #[Valid_tms, Node, Node, History]\n",
    "                else:\n",
    "                    self.normalized_traffic_matrices[tm_idx-idx_offset,:,:,h] = self.traffic_matrices[tm_idx-h]                         #[Valid_tms, Node, Node, History]\n",
    "\n",
    "    def get_topK_flows(self, tm_idx, pairs):\n",
    "        tm = self.traffic_matrices[tm_idx]\n",
    "        f = {}\n",
    "        for p in pairs:\n",
    "            s, d = self.pair_idx_to_sd[p]\n",
    "            f[p] = tm[s][d]\n",
    "\n",
    "        sorted_f = sorted(f.items(), key = lambda kv: (kv[1], kv[0]), reverse=True)\n",
    "\n",
    "        cf = []\n",
    "        for i in range(self.max_moves):\n",
    "            cf.append(sorted_f[i][0])\n",
    "\n",
    "        return cf\n",
    "       \n",
    "    def get_ecmp_next_hops(self):\n",
    "        self.ecmp_next_hops = {}\n",
    "        for src in range(self.num_nodes):\n",
    "            for dst in range(self.num_nodes):\n",
    "                if src == dst:\n",
    "                    continue\n",
    "                self.ecmp_next_hops[src, dst] = []\n",
    "                for p in self.shortest_paths_node[self.pair_sd_to_idx[(src, dst)]]:\n",
    "                    if p[1] not in self.ecmp_next_hops[src, dst]:\n",
    "                        self.ecmp_next_hops[src, dst].append(p[1])\n",
    "\n",
    "    def ecmp_next_hop_distribution(self, link_loads, demand, src, dst):\n",
    "        if src == dst:\n",
    "            return\n",
    "\n",
    "        ecmp_next_hops = self.ecmp_next_hops[src, dst]\n",
    "\n",
    "        next_hops_cnt = len(ecmp_next_hops)\n",
    "        #if next_hops_cnt > 1:\n",
    "            #print(self.shortest_paths_node[self.pair_sd_to_idx[(src, dst)]])\n",
    "\n",
    "        ecmp_demand = demand / next_hops_cnt \n",
    "        for np in ecmp_next_hops:\n",
    "            link_loads[self.link_sd_to_idx[(src, np)]] += ecmp_demand\n",
    "            self.ecmp_next_hop_distribution(link_loads, ecmp_demand, np, dst)\n",
    "\n",
    "    def ecmp_traffic_distribution(self, tm_idx):\n",
    "        link_loads = np.zeros((self.num_links))\n",
    "        tm = self.traffic_matrices[tm_idx]\n",
    "        for pair_idx in range(self.num_pairs):\n",
    "            s, d = self.pair_idx_to_sd[pair_idx]\n",
    "            demand = tm[s][d]\n",
    "            if demand != 0:\n",
    "                self.ecmp_next_hop_distribution(link_loads, demand, s, d)\n",
    "\n",
    "        return link_loads\n",
    "\n",
    "    def get_critical_topK_flows(self, tm_idx, critical_links=5):\n",
    "        link_loads = self.ecmp_traffic_distribution(tm_idx)\n",
    "        critical_link_indexes = np.argsort(-(link_loads / self.link_capacities))[:critical_links]\n",
    "        \n",
    "        cf_potential = []\n",
    "        for pair_idx in range(self.num_pairs):\n",
    "            for path in self.shortest_paths_link[pair_idx]:\n",
    "                if len(set(path).intersection(critical_link_indexes)) > 0:\n",
    "                    cf_potential.append(pair_idx)\n",
    "                    break\n",
    "\n",
    "        #print(cf_potential)\n",
    "        assert len(cf_potential) >= self.max_moves,                 (\"cf_potential(%d) < max_move(%d), please increse critical_links(%d)\"%(cf_potential, self.max_moves, critical_links))\n",
    "\n",
    "        return self.get_topK_flows(tm_idx, cf_potential)\n",
    "    def eval_ecmp_traffic_distribution2(self, tm_idx, look_ahead_window,eval_delay=False):\n",
    "        mlus = []\n",
    "        for tm in range(tm_idx,tm_idx+look_ahead_window):\n",
    "            eval_link_loads = self.ecmp_traffic_distribution(tm)\n",
    "            eval_max_utilization = np.max(eval_link_loads / self.link_capacities)\n",
    "            self.load_multiplier[tm] = 0.9 / eval_max_utilization\n",
    "            mlus.append(eval_max_utilization)\n",
    "        \n",
    "            \n",
    "        return mlus        \n",
    "    def eval_ecmp_traffic_distribution(self, tm_idx, eval_delay=False):\n",
    "        eval_link_loads = self.ecmp_traffic_distribution(tm_idx)\n",
    "        eval_max_utilization = np.max(eval_link_loads / self.link_capacities)\n",
    "        self.load_multiplier[tm_idx] = 0.9 / eval_max_utilization\n",
    "        delay = 0\n",
    "        if eval_delay:\n",
    "            eval_link_loads *= self.load_multiplier[tm_idx]\n",
    "            delay = sum(eval_link_loads / (self.link_capacities - eval_link_loads))\n",
    "\n",
    "        return eval_max_utilization, delay\n",
    "    def optimal_routing_mlu_default(self, tm_idx):\n",
    "        tm = self.traffic_matrices[tm_idx]\n",
    "\n",
    "        demands = {}\n",
    "        for i in range(self.num_pairs):\n",
    "            s, d = self.pair_idx_to_sd[i]\n",
    "            demands[i] = tm[s][d]\n",
    "            #print(\"demand from src %s to dst %s is %s\"%(s,d,demands[i])) \n",
    "\n",
    "        model = LpProblem(name=\"routing\")\n",
    "        #print('this %s is our self.pair_links' %(self.pair_links)) \n",
    "        ratio = LpVariable.dicts(name=\"ratio\", indexs=self.pair_links, lowBound=0, upBound=1)\n",
    "        #print(ratio)\n",
    "        link_load = LpVariable.dicts(name=\"link_load\", indexs=self.links)\n",
    "        #print(\"this %s is self.lp_links\"%(self.lp_links))\n",
    "        #print(\"this %s is self.lp_nodes\"%(self.lp_nodes))\n",
    "        #print(\"this %s is self.lp_pairs\"%(self.lp_pairs))\n",
    "        \"\"\"this will give MLU 0.5\"\"\"\n",
    "        each_flow_edges = {0:[(0,1)],1:[(0,2)],2:[(0,1),(1,3),(0,2),(2,3)],\n",
    "                          3:[(1,0)],4:[(1,0),(0,2)],5:[(1,3)],6:[(2,0)],\n",
    "                          7:[(2,0),(0,1)],8:[(2,3)],9:[(3,1),(1,0)],10:[(3,1)],11:[(3,2)]\n",
    "                          }\n",
    "        \"\"\"this will give MLU 1.0\"\"\"\n",
    "        each_flow_edges = {0: [(0, 1)],\n",
    "                           1: [(0, 2)], \n",
    "                           2: [(0, 1),(1,3),(0,2),(2,3)], \n",
    "                           3: [(1, 0)] ,\n",
    "                           4: [(1,0),(0, 2)], \n",
    "                           5: [(1, 3)],\n",
    "                           6: [(2,0)],\n",
    "                           7: [(2,0),(0 ,1)], \n",
    "                           8: [(2, 3)] ,\n",
    "                           9: [(3, 1),(1,0)], \n",
    "                           10: [(3,1)], \n",
    "                           11: [(3,2)] ,\n",
    "                           12: [(0,2),(2,4)] ,\n",
    "                           13: [(4,2),(2,0)] ,\n",
    "                           14: [(2, 4)] , \n",
    "                           15: [(4, 2)] , \n",
    "                           16: [(4, 2),(2,0),(0,1)] , \n",
    "                           17: [(4, 2),(2,3)] , \n",
    "                           18: [(1,0),(0, 2),(2,4)] , \n",
    "                           19: [(3, 2),(2,4)]\n",
    "                          }\n",
    "        r = LpVariable(name=\"congestion_ratio\")\n",
    "#         for pr in self.lp_pairs:\n",
    "#             for edge in self.lp_links:\n",
    "#                 #if (edge[0],edge[1]) not in each_flow_edges[pr]:\n",
    "#                 model +=(ratio[pr, edge[0], edge[1]] <=0.0)\n",
    "        \n",
    "        for pr in self.lp_pairs:\n",
    "#            print(\"for pr %s we add\"%(pr))\n",
    "            model += (lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[1] == self.pair_idx_to_sd[pr][0]]) - lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[0] == self.pair_idx_to_sd[pr][0]]) == -1 , \"flow_conservation_constr1_%d\"%pr)\n",
    "\n",
    "        for pr in self.lp_pairs:\n",
    "            model += (lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[1] == self.pair_idx_to_sd[pr][1]]) - lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[0] == self.pair_idx_to_sd[pr][1]]) == 1, \"flow_conservation_constr2_%d\"%pr)\n",
    "\n",
    "        for pr in self.lp_pairs:\n",
    "            for n in self.lp_nodes:\n",
    "                if n not in self.pair_idx_to_sd[pr]:\n",
    "                    model += (lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[1] == n]) - lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[0] == n]) == 0, \"flow_conservation_constr3_%d_%d\"%(pr,n))\n",
    "\n",
    "        for e in self.lp_links:\n",
    "            ei = self.link_sd_to_idx[e]\n",
    "            #print(\"for edge %s ei %s capacity is %s \"%(e,ei,self.link_capacities[ei]))\n",
    "            model += (link_load[ei] == lpSum([demands[pr]*ratio[pr, e[0], e[1]] for pr in self.lp_pairs]), \"link_load_constr%d\"%ei)\n",
    "            model += (link_load[ei] <= self.link_capacities[ei]*r, \"congestion_ratio_constr%d\"%ei)\n",
    "\n",
    "        model += r + OBJ_EPSILON*lpSum([link_load[e] for e in self.links])\n",
    "\n",
    "        model.solve(solver=GLPK(msg=False))\n",
    "        assert LpStatus[model.status] == 'Optimal'\n",
    "\n",
    "        obj_r = r.value()\n",
    "        solution = {}\n",
    "        for k in ratio:\n",
    "            solution[k] = ratio[k].value()\n",
    "\n",
    "        return obj_r, solution\n",
    "    def mlu_optimal_routing_mlu(self, tm_idx,look_ahead_window):\n",
    "        mlus= []\n",
    "        solutions = []\n",
    "        for traffic_m_indx in range(tm_idx,tm_idx+look_ahead_window-1):\n",
    "            tm = self.traffic_matrices[traffic_m_indx]\n",
    "\n",
    "            demands = {}\n",
    "            for i in range(self.num_pairs):\n",
    "                s, d = self.pair_idx_to_sd[i]\n",
    "                demands[i] = tm[s][d]\n",
    "                #print(\"demand from src %s to dst %s is %s\"%(s,d,demands[i])) \n",
    "\n",
    "            model = LpProblem(name=\"routing\")\n",
    "            #print('this %s is our self.pair_links' %(self.pair_links)) \n",
    "            ratio = LpVariable.dicts(name=\"ratio\", indexs=self.pair_links, lowBound=0, upBound=1)\n",
    "            #print(ratio)\n",
    "            link_load = LpVariable.dicts(name=\"link_load\", indexs=self.links)\n",
    "            #print(\"this %s is self.lp_links\"%(self.lp_links))\n",
    "            #print(\"this %s is self.lp_nodes\"%(self.lp_nodes))\n",
    "            #print(\"this %s is self.lp_pairs\"%(self.lp_pairs))\n",
    "            \"\"\"this will give MLU 0.5\"\"\"\n",
    "            each_flow_edges = {0:[(0,1)],1:[(0,2)],2:[(0,1),(1,3),(0,2),(2,3)],\n",
    "                              3:[(1,0)],4:[(1,0),(0,2)],5:[(1,3)],6:[(2,0)],\n",
    "                              7:[(2,0),(0,1)],8:[(2,3)],9:[(3,1),(1,0)],10:[(3,1)],11:[(3,2)]\n",
    "                              }\n",
    "            \"\"\"this will give MLU 1.0\"\"\"\n",
    "            each_flow_edges = {0: [(0, 1)],\n",
    "                               1: [(0, 2)], \n",
    "                               2: [(0, 1),(1,3),(0,2),(2,3)], \n",
    "                               3: [(1, 0)] ,\n",
    "                               4: [(1,0),(0, 2)], \n",
    "                               5: [(1, 3)],\n",
    "                               6: [(2,0)],\n",
    "                               7: [(2,0),(0 ,1)], \n",
    "                               8: [(2, 3)] ,\n",
    "                               9: [(3, 1),(1,0)], \n",
    "                               10: [(3,1)], \n",
    "                               11: [(3,2)] ,\n",
    "                               12: [(0,2),(2,4)] ,\n",
    "                               13: [(4,2),(2,0)] ,\n",
    "                               14: [(2, 4)] , \n",
    "                               15: [(4, 2)] , \n",
    "                               16: [(4, 2),(2,0),(0,1)] , \n",
    "                               17: [(4, 2),(2,3)] , \n",
    "                               18: [(1,0),(0, 2),(2,4)] , \n",
    "                               19: [(3, 2),(2,4)]\n",
    "                              }\n",
    "            r = LpVariable(name=\"congestion_ratio\")\n",
    "    #         for pr in self.lp_pairs:\n",
    "    #             for edge in self.lp_links:\n",
    "    #                 if (edge[0],edge[1]) not in each_flow_edges[pr]:\n",
    "    #                     model +=(ratio[pr, edge[0], edge[1]] <=0.0)\n",
    "\n",
    "            for pr in self.lp_pairs:\n",
    "    #            print(\"for pr %s we add\"%(pr))\n",
    "                model += (lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[1] == self.pair_idx_to_sd[pr][0]]) - lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[0] == self.pair_idx_to_sd[pr][0]]) == -1 , \"flow_conservation_constr1_%d\"%pr)\n",
    "\n",
    "            for pr in self.lp_pairs:\n",
    "                model += (lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[1] == self.pair_idx_to_sd[pr][1]]) - lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[0] == self.pair_idx_to_sd[pr][1]]) == 1, \"flow_conservation_constr2_%d\"%pr)\n",
    "\n",
    "            for pr in self.lp_pairs:\n",
    "                for n in self.lp_nodes:\n",
    "                    if n not in self.pair_idx_to_sd[pr]:\n",
    "                        model += (lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[1] == n]) - lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[0] == n]) == 0, \"flow_conservation_constr3_%d_%d\"%(pr,n))\n",
    "\n",
    "            for e in self.lp_links:\n",
    "                ei = self.link_sd_to_idx[e]\n",
    "                #print(\"for edge %s ei %s capacity is %s \"%(e,ei,self.link_capacities[ei]))\n",
    "                model += (link_load[ei] == lpSum([demands[pr]*ratio[pr, e[0], e[1]] for pr in self.lp_pairs]), \"link_load_constr%d\"%ei)\n",
    "                model += (link_load[ei] <= self.link_capacities[ei]*r, \"congestion_ratio_constr%d\"%ei)\n",
    "\n",
    "            model += r + OBJ_EPSILON*lpSum([link_load[e] for e in self.links])\n",
    "\n",
    "            model.solve(solver=GLPK(msg=False))\n",
    "            assert LpStatus[model.status] == 'Optimal'\n",
    "\n",
    "            obj_r = r.value()\n",
    "            solution = {}\n",
    "            for k in ratio:\n",
    "                solution[k] = ratio[k].value()\n",
    "            mlu, optimal_mlu_delay = self.eval_optimal_routing_mlu(traffic_m_indx, solution, eval_delay=False)\n",
    "            mlus.append(mlu)\n",
    "            solutions.append(solution)\n",
    "        return mlus,solutions\n",
    "    def mlu_routing_selected_paths_oblivious(self,tm_idx,look_ahead_window,each_flow_edges):\n",
    "        \n",
    "        #print(\"we check mlu for tm_idx\",tm_idx)\n",
    "        tm_mlu_using_suggested_paths = []\n",
    "        solutions = []\n",
    "        #for k in each_flow_edges.keys():\n",
    "            #print('flow is ',k)\n",
    "        for i in range(self.num_pairs):\n",
    "            s, d = self.pair_idx_to_sd[i]\n",
    "            #print('flow is ',s,d)\n",
    "        #print('self.lp_pairs',self.lp_pairs)\n",
    "        #print(each_flow_edges)\n",
    "        import pdb\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        traffic_m_indx = tm_idx\n",
    "        tm = self.traffic_matrices[traffic_m_indx]\n",
    "\n",
    "        demands = {}\n",
    "        for i in range(self.num_pairs):\n",
    "            s, d = self.pair_idx_to_sd[i]\n",
    "            demands[i] = tm[s][d]\n",
    "            #demands[i] = 0\n",
    "            #print(\"demand from src %s to dst %s is %s\"%(s,d,demands[i])) \n",
    "\n",
    "        model = LpProblem(name=\"routing\")\n",
    "#         print('this %s is our self.pair_links' %(self.pair_links)) \n",
    "        ratio = LpVariable.dicts(name=\"ratio\", indexs=self.pair_links, lowBound=0, upBound=1)\n",
    "        #print(ratio)\n",
    "        link_load = LpVariable.dicts(name=\"link_load\", indexs=self.links)\n",
    "#         print(\"this %s is self.lp_links\"%(self.lp_links))\n",
    "#         print(\"this %s is self.lp_nodes\"%(self.lp_nodes))\n",
    "#         print(\"this %s is self.lp_pairs\"%(self.lp_pairs))\n",
    "        import pdb\n",
    "\n",
    "        r = LpVariable(name=\"congestion_ratio\")\n",
    "        for pr in self.lp_pairs:\n",
    "            s, d = self.pair_idx_to_sd[pr]\n",
    "            for edge in self.lp_links:\n",
    "                if (edge[0],edge[1]) not in each_flow_edges[(s,d)]:\n",
    "                    model +=(ratio[pr, edge[0], edge[1]] ==0.0)\n",
    "\n",
    "        for pr in self.lp_pairs:\n",
    "#            print(\"for pr %s we add\"%(pr))\n",
    "            #s, d = self.pair_idx_to_sd[pr]\n",
    "            model += (lpSum([ratio[pr, e[0], e[1]] \n",
    "                             for e in self.lp_links if e[1] == self.pair_idx_to_sd[pr][0]]) - \n",
    "                      lpSum([ratio[pr, e[0], e[1]] \n",
    "                        for e in self.lp_links if e[0] == self.pair_idx_to_sd[pr][0]]) == -1 ,\n",
    "                      \"flow_conservation_constr1_%d\"%pr)\n",
    "\n",
    "        for pr in self.lp_pairs:\n",
    "            model += (lpSum([ratio[pr, e[0], e[1]] \n",
    "                    for e in self.lp_links if e[1] == self.pair_idx_to_sd[pr][1]]) -\n",
    "                    lpSum([ratio[pr, e[0], e[1]] \n",
    "                    for e in self.lp_links if e[0] == self.pair_idx_to_sd[pr][1]]) == 1,\n",
    "                      \"flow_conservation_constr2_%d\"%pr)\n",
    "\n",
    "        for pr in self.lp_pairs:\n",
    "            for n in self.lp_nodes:\n",
    "                if n not in self.pair_idx_to_sd[pr]:\n",
    "                    model += (lpSum([ratio[pr, e[0], e[1]] \n",
    "                    for e in self.lp_links if e[1] == n]) -\n",
    "                    lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[0] == n]) == 0,\n",
    "                    \"flow_conservation_constr3_%d_%d\"%(pr,n))\n",
    "\n",
    "        for e in self.lp_links:\n",
    "            ei = self.link_sd_to_idx[e]\n",
    "            #print(\"for edge %s ei %s capacity is %s \"%(e,ei,self.link_capacities[ei]))\n",
    "            model += (link_load[ei] == lpSum([demands[pr]*ratio[pr, e[0], e[1]] \n",
    "                for pr in self.lp_pairs]), \"link_load_constr%d\"%ei)\n",
    "\n",
    "            model += (link_load[ei] <= self.link_capacities[ei]*r, \"congestion_ratio_constr%d\"%ei)\n",
    "\n",
    "        model += r + OBJ_EPSILON*lpSum([link_load[e] for e in self.links])\n",
    "\n",
    "        model.solve(solver=GLPK(msg=False))\n",
    "        assert LpStatus[model.status] == 'Optimal'\n",
    "\n",
    "        obj_r = r.value()\n",
    "        solution = {}\n",
    "        for k in ratio:\n",
    "            solution[k] = ratio[k].value()\n",
    "        for traffic_m_indx in range(tm_idx,tm_idx+look_ahead_window-1):\n",
    "            tm = self.traffic_matrices[traffic_m_indx]\n",
    "            oblivious_mlu, oblivious_mlu_delay = self.eval_optimal_routing_mlu(traffic_m_indx, solution, eval_delay=False)\n",
    "            tm_mlu_using_suggested_paths.append(oblivious_mlu)\n",
    "            solutions.append(solution)\n",
    "            #print(\"******* we got the solution \",drl_mlu)\n",
    "        #return obj_r, solution \n",
    "        #print(\"here are the four mlu\",tm_mlu_using_suggested_paths)\n",
    "        return tm_mlu_using_suggested_paths,solutions\n",
    "    def mlu_routing_selected_paths(self,tm_idx,look_ahead_window,each_flow_edges):\n",
    "        #print(\"we check mlu for tm_idx\",tm_idx)\n",
    "        tm_mlu_using_suggested_paths = []\n",
    "        solutions = []\n",
    "        #for k in each_flow_edges.keys():\n",
    "            #print('flow is ',k)\n",
    "        for i in range(self.num_pairs):\n",
    "            s, d = self.pair_idx_to_sd[i]\n",
    "            #print('flow is ',s,d)\n",
    "        #print('self.lp_pairs',self.lp_pairs)\n",
    "        #print(each_flow_edges)\n",
    "        import pdb\n",
    "        #pdb.set_trace()\n",
    "        for traffic_m_indx in range(tm_idx,tm_idx+look_ahead_window-1):\n",
    "            tm = self.traffic_matrices[traffic_m_indx]\n",
    "\n",
    "            demands = {}\n",
    "            for i in range(self.num_pairs):\n",
    "                s, d = self.pair_idx_to_sd[i]\n",
    "                demands[i] = tm[s][d]\n",
    "                #demands[i] = 0\n",
    "                #print(\"demand from src %s to dst %s is %s\"%(s,d,demands[i])) \n",
    "\n",
    "            model = LpProblem(name=\"routing\")\n",
    "    #         print('this %s is our self.pair_links' %(self.pair_links)) \n",
    "            ratio = LpVariable.dicts(name=\"ratio\", indexs=self.pair_links, lowBound=0, upBound=1)\n",
    "            #print(ratio)\n",
    "            link_load = LpVariable.dicts(name=\"link_load\", indexs=self.links)\n",
    "    #         print(\"this %s is self.lp_links\"%(self.lp_links))\n",
    "    #         print(\"this %s is self.lp_nodes\"%(self.lp_nodes))\n",
    "    #         print(\"this %s is self.lp_pairs\"%(self.lp_pairs))\n",
    "            import pdb\n",
    "            \n",
    "            r = LpVariable(name=\"congestion_ratio\")\n",
    "            for pr in self.lp_pairs:\n",
    "                s, d = self.pair_idx_to_sd[pr]\n",
    "                for edge in self.lp_links:\n",
    "                    if (edge[0],edge[1]) not in each_flow_edges[(s,d)]:\n",
    "                        model +=(ratio[pr, edge[0], edge[1]] ==0.0)\n",
    "\n",
    "            for pr in self.lp_pairs:\n",
    "    #            print(\"for pr %s we add\"%(pr))\n",
    "                #s, d = self.pair_idx_to_sd[pr]\n",
    "                model += (lpSum([ratio[pr, e[0], e[1]] \n",
    "                                 for e in self.lp_links if e[1] == self.pair_idx_to_sd[pr][0]]) - \n",
    "                          lpSum([ratio[pr, e[0], e[1]] \n",
    "                            for e in self.lp_links if e[0] == self.pair_idx_to_sd[pr][0]]) == -1 ,\n",
    "                          \"flow_conservation_constr1_%d\"%pr)\n",
    "\n",
    "            for pr in self.lp_pairs:\n",
    "                model += (lpSum([ratio[pr, e[0], e[1]] \n",
    "                        for e in self.lp_links if e[1] == self.pair_idx_to_sd[pr][1]]) -\n",
    "                        lpSum([ratio[pr, e[0], e[1]] \n",
    "                        for e in self.lp_links if e[0] == self.pair_idx_to_sd[pr][1]]) == 1,\n",
    "                          \"flow_conservation_constr2_%d\"%pr)\n",
    "\n",
    "            for pr in self.lp_pairs:\n",
    "                for n in self.lp_nodes:\n",
    "                    if n not in self.pair_idx_to_sd[pr]:\n",
    "                        model += (lpSum([ratio[pr, e[0], e[1]] \n",
    "                        for e in self.lp_links if e[1] == n]) -\n",
    "                        lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[0] == n]) == 0,\n",
    "                        \"flow_conservation_constr3_%d_%d\"%(pr,n))\n",
    "\n",
    "            for e in self.lp_links:\n",
    "                ei = self.link_sd_to_idx[e]\n",
    "                #print(\"for edge %s ei %s capacity is %s \"%(e,ei,self.link_capacities[ei]))\n",
    "                model += (link_load[ei] == lpSum([demands[pr]*ratio[pr, e[0], e[1]] \n",
    "                    for pr in self.lp_pairs]), \"link_load_constr%d\"%ei)\n",
    "\n",
    "                model += (link_load[ei] <= self.link_capacities[ei]*r, \"congestion_ratio_constr%d\"%ei)\n",
    "\n",
    "            model += r + OBJ_EPSILON*lpSum([link_load[e] for e in self.links])\n",
    "\n",
    "            model.solve(solver=GLPK(msg=False))\n",
    "            assert LpStatus[model.status] == 'Optimal'\n",
    "\n",
    "            obj_r = r.value()\n",
    "            solution = {}\n",
    "            for k in ratio:\n",
    "                solution[k] = ratio[k].value()\n",
    "            drl_mlu, optimal_mlu_delay = self.eval_optimal_routing_mlu(traffic_m_indx, solution, eval_delay=False)\n",
    "            tm_mlu_using_suggested_paths.append(drl_mlu)\n",
    "            solutions.append(solution)\n",
    "            #print(\"******* we got the solution \",drl_mlu)\n",
    "        #return obj_r, solution \n",
    "        #print(\"here are the four mlu\",tm_mlu_using_suggested_paths)\n",
    "        return tm_mlu_using_suggested_paths,solutions\n",
    "    def eval_optimal_routing_mlu(self, tm_idx, solution, eval_delay=False):\n",
    "        optimal_link_loads = np.zeros((self.num_links))\n",
    "        eval_tm = self.traffic_matrices[tm_idx]\n",
    "        for i in range(self.num_pairs):\n",
    "            s, d = self.pair_idx_to_sd[i]\n",
    "            demand = eval_tm[s][d]\n",
    "            for e in self.lp_links:\n",
    "                link_idx = self.link_sd_to_idx[e]\n",
    "                #print(\"for src %s dst %s  e %s we have demand %s and solution %s \"%(s,d,e,demand,solution[i, e[0], e[1]]))\n",
    "                optimal_link_loads[link_idx] += demand*solution[i, e[0], e[1]]\n",
    "        \n",
    "        optimal_max_utilization = np.max(optimal_link_loads / self.link_capacities)\n",
    "        delay = 0\n",
    "        if eval_delay:\n",
    "            assert tm_idx in self.load_multiplier, (tm_idx)\n",
    "            optimal_link_loads *= self.load_multiplier[tm_idx]\n",
    "            delay = sum(optimal_link_loads / (self.link_capacities - optimal_link_loads))\n",
    "                        \n",
    "        return optimal_max_utilization, delay\n",
    "\n",
    "    def optimal_routing_mlu_critical_pairs(self, tm_idx, critical_pairs):\n",
    "        tm = self.traffic_matrices[tm_idx]\n",
    "\n",
    "        pairs = critical_pairs\n",
    "\n",
    "        demands = {}\n",
    "        background_link_loads = np.zeros((self.num_links))\n",
    "        for i in range(self.num_pairs):\n",
    "            s, d = self.pair_idx_to_sd[i]\n",
    "            #background link load\n",
    "            if i not in critical_pairs:\n",
    "                self.ecmp_next_hop_distribution(background_link_loads, tm[s][d], s, d)\n",
    "            else:\n",
    "                demands[i] = tm[s][d]\n",
    "\n",
    "        model = LpProblem(name=\"routing\")\n",
    "        \n",
    "        pair_links = [(pr, e[0], e[1]) for pr in pairs for e in self.lp_links] \n",
    "        ratio = LpVariable.dicts(name=\"ratio\", indexs=pair_links, lowBound=0, upBound=1)\n",
    "        \n",
    "        link_load = LpVariable.dicts(name=\"link_load\", indexs=self.links)\n",
    "\n",
    "        r = LpVariable(name=\"congestion_ratio\")\n",
    "\n",
    "        for pr in pairs:\n",
    "            model += (lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[1] == self.pair_idx_to_sd[pr][0]]) - lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[0] == self.pair_idx_to_sd[pr][0]]) == -1, \"flow_conservation_constr1_%d\"%pr)\n",
    "\n",
    "        for pr in pairs:\n",
    "            model += (lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[1] == self.pair_idx_to_sd[pr][1]]) - lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[0] == self.pair_idx_to_sd[pr][1]]) == 1, \"flow_conservation_constr2_%d\"%pr)\n",
    "\n",
    "        for pr in pairs:\n",
    "            for n in self.lp_nodes:\n",
    "                if n not in self.pair_idx_to_sd[pr]:\n",
    "                    model += (lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[1] == n]) - lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[0] == n]) == 0, \"flow_conservation_constr3_%d_%d\"%(pr,n))\n",
    "\n",
    "        for e in self.lp_links:\n",
    "            ei = self.link_sd_to_idx[e]\n",
    "            model += (link_load[ei] == background_link_loads[ei] + lpSum([demands[pr]*ratio[pr, e[0], e[1]] for pr in pairs]), \"link_load_constr%d\"%ei)\n",
    "            model += (link_load[ei] <= self.link_capacities[ei]*r, \"congestion_ratio_constr%d\"%ei)\n",
    "\n",
    "        model += r + OBJ_EPSILON*lpSum([link_load[ei] for ei in self.links])\n",
    "\n",
    "        model.solve(solver=GLPK(msg=False))\n",
    "        assert LpStatus[model.status] == 'Optimal'\n",
    "\n",
    "        obj_r = r.value()\n",
    "        solution = {}\n",
    "        for k in ratio:\n",
    "            solution[k] = ratio[k].value()\n",
    "\n",
    "        return obj_r, solution\n",
    "\n",
    "    def eval_critical_flow_and_ecmp(self, tm_idx, critical_pairs, solution, eval_delay=False):\n",
    "        eval_tm = self.traffic_matrices[tm_idx]\n",
    "        eval_link_loads = np.zeros((self.num_links))\n",
    "        for i in range(self.num_pairs):\n",
    "            s, d = self.pair_idx_to_sd[i]\n",
    "            if i not in critical_pairs:\n",
    "                self.ecmp_next_hop_distribution(eval_link_loads, eval_tm[s][d], s, d)\n",
    "            else:\n",
    "                demand = eval_tm[s][d]\n",
    "                for e in self.lp_links:\n",
    "                    link_idx = self.link_sd_to_idx[e]\n",
    "                    eval_link_loads[link_idx] += eval_tm[s][d]*solution[i, e[0], e[1]]\n",
    "\n",
    "        eval_max_utilization = np.max(eval_link_loads / self.link_capacities)\n",
    "        delay = 0\n",
    "        if eval_delay:\n",
    "            assert tm_idx in self.load_multiplier, (tm_idx)\n",
    "            eval_link_loads *= self.load_multiplier[tm_idx]\n",
    "            delay = sum(eval_link_loads / (self.link_capacities - eval_link_loads))\n",
    "        \n",
    "        return eval_max_utilization, delay\n",
    "\n",
    "    def optimal_routing_delay(self, tm_idx):\n",
    "        assert tm_idx in self.load_multiplier, (tm_idx)\n",
    "        tm = self.traffic_matrices[tm_idx]*self.load_multiplier[tm_idx]\n",
    "        demands = {}\n",
    "        for i in range(self.num_pairs):\n",
    "            s, d = self.pair_idx_to_sd[i]\n",
    "            demands[i] = tm[s][d]\n",
    "\n",
    "        model = LpProblem(name=\"routing\")\n",
    "     \n",
    "        ratio = LpVariable.dicts(name=\"ratio\", indexs=self.pair_links, lowBound=0, upBound=1)\n",
    "\n",
    "        link_load = LpVariable.dicts(name=\"link_load\", indexs=self.links)\n",
    "\n",
    "        f = LpVariable.dicts(name=\"link_cost\", indexs=self.links)\n",
    "\n",
    "        for pr in self.lp_pairs:\n",
    "            model += (lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[1] == self.pair_idx_to_sd[pr][0]]) - lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[0] == self.pair_idx_to_sd[pr][0]]) == -1, \"flow_conservation_constr1_%d\"%pr)\n",
    "\n",
    "        for pr in self.lp_pairs:\n",
    "            model += (lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[1] == self.pair_idx_to_sd[pr][1]]) - lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[0] == self.pair_idx_to_sd[pr][1]]) == 1, \"flow_conservation_constr2_%d\"%pr)\n",
    "\n",
    "        for pr in self.lp_pairs:\n",
    "            for n in self.lp_nodes:\n",
    "                if n not in self.pair_idx_to_sd[pr]:\n",
    "                    model += (lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[1] == n]) - lpSum([ratio[pr, e[0], e[1]] for e in self.lp_links if e[0] == n]) == 0, \"flow_conservation_constr3_%d_%d\"%(pr,n))\n",
    "\n",
    "        for e in self.lp_links:\n",
    "            ei = self.link_sd_to_idx[e]\n",
    "            model += (link_load[ei] == lpSum([demands[pr]*ratio[pr, e[0], e[1]] for pr in self.lp_pairs]), \"link_load_constr%d\"%ei)\n",
    "            model += (f[ei] * self.link_capacities[ei] >= link_load[ei], \"cost_constr1_%d\"%ei)\n",
    "            model += (f[ei] >= 3 * link_load[ei] / self.link_capacities[ei] - 2/3, \"cost_constr2_%d\"%ei)\n",
    "            model += (f[ei] >= 10 * link_load[ei] / self.link_capacities[ei] - 16/3, \"cost_constr3_%d\"%ei)\n",
    "            model += (f[ei] >= 70 * link_load[ei] / self.link_capacities[ei] - 178/3, \"cost_constr4_%d\"%ei)\n",
    "            model += (f[ei] >= 500 * link_load[ei] / self.link_capacities[ei] - 1468/3, \"cost_constr5_%d\"%ei)\n",
    "            model += (f[ei] >= 5000 * link_load[ei] / self.link_capacities[ei] - 16318/3, \"cost_constr6_%d\"%ei)\n",
    "       \n",
    "        model += lpSum(f[ei] for ei in self.links)\n",
    "\n",
    "        model.solve(solver=GLPK(msg=False))\n",
    "        assert LpStatus[model.status] == 'Optimal'\n",
    "\n",
    "        solution = {}\n",
    "        for k in ratio:\n",
    "            solution[k] = ratio[k].value()\n",
    "\n",
    "        return solution\n",
    "\n",
    "    def eval_optimal_routing_delay(self, tm_idx, solution):\n",
    "        optimal_link_loads = np.zeros((self.num_links))\n",
    "        assert tm_idx in self.load_multiplier, (tm_idx)\n",
    "        eval_tm = self.traffic_matrices[tm_idx]*self.load_multiplier[tm_idx]\n",
    "        for i in range(self.num_pairs):\n",
    "            s, d = self.pair_idx_to_sd[i]\n",
    "            demand = eval_tm[s][d]\n",
    "            for e in self.lp_links:\n",
    "                link_idx = self.link_sd_to_idx[e]\n",
    "                optimal_link_loads[link_idx] += demand*solution[i, e[0], e[1]]\n",
    "        \n",
    "        optimal_delay = sum(optimal_link_loads / (self.link_capacities - optimal_link_loads))\n",
    "\n",
    "        return optimal_delay\n",
    "    def get_suggested_paths_by_rl(self,actions,each_flow_paths,each_path_id,each_id_path,each_flow_shortest_path):\n",
    "        \n",
    "        for flow,paths in each_flow_paths.items():\n",
    "            covered_flow = False\n",
    "            for path in paths:\n",
    "                path_id = each_path_id[path]\n",
    "                if not covered_flow:\n",
    "                    if path_id in actions:\n",
    "                        covered_flow =True \n",
    "            if not covered_flow:\n",
    "                #print(\"flow  did not have any candidate path in chosen paths\",flow)\n",
    "                flow_shortest_path = each_flow_shortest_path[flow]\n",
    "                #print(flow_shortest_path)\n",
    "                #print(each_path_id)\n",
    "                path_id = each_path_id[tuple(flow_shortest_path)]\n",
    "\n",
    "                actions = np.append(actions, path_id)\n",
    "        each_flow_edges = {}\n",
    "        for flow in each_flow_shortest_path:\n",
    "            for path_id in actions:\n",
    "                path = each_id_path[path_id]\n",
    "                if path in each_flow_paths[flow]:\n",
    "                    for node_indx in range(len(path)-1):\n",
    "                        try:\n",
    "                            if (path[node_indx],path[node_indx+1]) not in each_flow_edges[flow]:\n",
    "                                each_flow_edges[flow].append((path[node_indx],path[node_indx+1]))\n",
    "                                each_flow_edges[flow].append((path[node_indx+1],path[node_indx]))\n",
    "                        except:\n",
    "                            each_flow_edges[flow]=[(path[node_indx],path[node_indx+1])]\n",
    "                            each_flow_edges[flow].append((path[node_indx+1],path[node_indx]))\n",
    "        return each_flow_edges\n",
    "    def get_hindsight_set_of_paths(self,tm_idx,look_ahead_window,each_flow_paths,each_path_id,each_id_path,each_flow_shortest_path,topology_file,config):\n",
    "        target_times = []\n",
    "        for t in range(tm_idx,tm_idx+look_ahead_window):\n",
    "            target_times.append(t)\n",
    "        each_path_counter = {}\n",
    "        #print('topology is ',topology_file)\n",
    "        with open(config.each_topology_each_t_each_f_paths) as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                if topology_file in line:#ATT_topology_file_modified:189:1->20: ['1', '13', '9', '4', '20'],['1', '9', '4', '20']\n",
    "                    time = line.split(\":\")[1]\n",
    "                    if int(time) in target_times:\n",
    "                        flow_str = line.split(\":\")[2]\n",
    "                        flow = flow_str.split(\"->\")\n",
    "                        flow = (int(flow[0]),int(flow[1]))\n",
    "                        paths = line.split(flow_str+\":\")[1]\n",
    "                        paths = paths.strip()\n",
    "                        paths = \"\\\"\"+str(paths)+\"\\\"\"\n",
    "                        paths = ast.literal_eval(paths)\n",
    "                        #print('time %s flow %s paths %s'%(time,flow, paths))\n",
    "                        #print(type(paths),len(paths))\n",
    "                        if '],[' in paths:\n",
    "                            paths = ast.literal_eval(paths)\n",
    "                        else:\n",
    "                            paths = ast.literal_eval(paths)\n",
    "                            new_path = []\n",
    "                            for p in paths:\n",
    "                                new_path.append(p)\n",
    "                            paths = [new_path]\n",
    "                        #print('2time %s flow %s paths %s'%(time,flow, paths))\n",
    "                        #print(type(paths),len(paths))\n",
    "                        for p in paths:\n",
    "                            p = tuple(p)\n",
    "                            new_p= []\n",
    "                            for node in p:\n",
    "                                new_p.append(int(node))\n",
    "                            p = tuple(new_p)\n",
    "                            try:\n",
    "                                each_path_counter[p]+=1\n",
    "                            except:\n",
    "                                each_path_counter[p]=1\n",
    "        repeated_times = []\n",
    "        for path,counter in   each_path_counter.items():\n",
    "            if counter not in repeated_times:\n",
    "                repeated_times.append(counter)\n",
    "        repeated_times.sort()\n",
    "        top_selected_path_ids = []\n",
    "        top_selected_counters = repeated_times[-self.max_moves:]\n",
    "        for p,counter in each_path_counter.items():\n",
    "            if counter in top_selected_counters:\n",
    "                if len(top_selected_path_ids)<self.max_moves:\n",
    "                    path_id = each_path_id[p]\n",
    "                    top_selected_path_ids.append(path_id)\n",
    "        \n",
    "        for flow,paths in each_flow_paths.items():\n",
    "            covered_flow = False\n",
    "            for path in paths:\n",
    "                path_id = each_path_id[path]\n",
    "                if not covered_flow:\n",
    "                    if path_id in top_selected_path_ids:\n",
    "                        covered_flow =True \n",
    "            if not covered_flow:\n",
    "                #print(\"flow  did not have any candidate path in chosen paths\",flow)\n",
    "                flow_shortest_path = each_flow_shortest_path[flow]\n",
    "                #print(flow_shortest_path)\n",
    "                #print(each_path_id)\n",
    "                path_id = each_path_id[tuple(flow_shortest_path)]\n",
    "                \n",
    "                top_selected_path_ids.append(path_id)\n",
    "                #print(\"we added path id %s for safety\"%(path_id))\n",
    "        \n",
    "        \"\"\"end of safe online learning section\"\"\"\n",
    "        \n",
    "#         for path_id in actions:\n",
    "#             print(\"we have chosen and safed action %s %s\"%(path_id,len(actions)))\n",
    "            \n",
    "        each_flow_selected_paths = {}\n",
    "        \"\"\"we now add the edges for each flow from the selected actions\"\"\"\n",
    "        each_flow_edges = {}\n",
    "        for flow in each_flow_shortest_path:\n",
    "            for path_id in top_selected_path_ids:\n",
    "                path = each_id_path[path_id]\n",
    "                if path in each_flow_paths[flow]:\n",
    "                    try:\n",
    "                        each_flow_selected_paths[flow].append(path)\n",
    "                    except:\n",
    "                        each_flow_selected_paths[flow]=[path]\n",
    "                    for node_indx in range(len(path)-1):\n",
    "                        try:\n",
    "                            if (path[node_indx],path[node_indx+1]) not in each_flow_edges[flow]:\n",
    "                                each_flow_edges[flow].append((path[node_indx],path[node_indx+1]))\n",
    "                                each_flow_edges[flow].append((path[node_indx+1],path[node_indx]))\n",
    "                        except:\n",
    "                            each_flow_edges[flow]=[(path[node_indx],path[node_indx+1])]\n",
    "                            each_flow_edges[flow].append((path[node_indx+1],path[node_indx]))\n",
    "        \n",
    "#         for flow,edges in each_flow_edges.items():\n",
    "#             print(\"this flow %s has these edges %s\"%(flow,edges))\n",
    "#         print(\"and this is our max moves\",self.max_moves)\n",
    "        return each_flow_edges\n",
    "# In[ ]:\n",
    "\n",
    "    def get_each_flow_paths_and_path_id(self,topology_name):\n",
    "        \"\"\"in this function, we get the B=4 most used paths by each flow\n",
    "\n",
    "        we get all the used path by each flow over all times and get top 4 of them\"\"\"\n",
    "        path_counter = 0\n",
    "        each_path_id = {}\n",
    "        set_of_times = set([])\n",
    "        each_flow_path_usage = {}\n",
    "        all_the_paths = set([])\n",
    "        each_t_paths = {}\n",
    "        each_flow_paths = {}\n",
    "        #print('topology is ',topology_name)\n",
    "        with open('each_topology_each_t_each_f_paths.txt') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                if topology_name in line:#ATT_topology_file_modified:189:1->20: ['1', '13', '9', '4', '20'],['1', '9', '4', '20']\n",
    "                    time = line.split(\":\")[1]\n",
    "                    flow_str = line.split(\":\")[2]\n",
    "                    flow = flow_str.split(\"->\")\n",
    "                    flow = (int(flow[0]),int(flow[1]))\n",
    "                    set_of_times.add(time)\n",
    "                    paths = line.split(flow_str+\":\")[1]\n",
    "                    paths = paths.strip()\n",
    "                    paths = \"\\\"\"+str(paths)+\"\\\"\"\n",
    "                    paths = ast.literal_eval(paths)\n",
    "                    #print('time %s flow %s paths %s'%(time,flow, paths))\n",
    "                    #print(type(paths),len(paths))\n",
    "                    if '],[' in paths:\n",
    "                        paths = ast.literal_eval(paths)\n",
    "                    else:\n",
    "                        paths = ast.literal_eval(paths)\n",
    "                        new_path = []\n",
    "                        for p in paths:\n",
    "                            new_path.append(p)\n",
    "                        paths = [new_path]\n",
    "                    #print('2time %s flow %s paths %s'%(time,flow, paths))\n",
    "                    #print(type(paths),len(paths))\n",
    "                    for p in paths:\n",
    "                        p = tuple(p)\n",
    "                        new_p= []\n",
    "                        for node in p:\n",
    "                            new_p.append(int(node))\n",
    "                        p = tuple(new_p)\n",
    "                        try:\n",
    "                            each_flow_paths[flow].add(p)\n",
    "                        except:\n",
    "                            each_flow_paths[flow] =set([p])\n",
    "                        try:\n",
    "                            each_t_paths[time].add(p)\n",
    "                        except:\n",
    "                            each_t_paths[time] = set([p])\n",
    "                        all_the_paths.add(p)\n",
    "                        #print('this is a path ',p)\n",
    "        each_path_id = {}\n",
    "        id_counter = 0\n",
    "        each_id_path = {}\n",
    "        for flow,paths in each_flow_paths.items():\n",
    "            for p in paths:\n",
    "                each_path_id[tuple(p)] = id_counter\n",
    "                each_id_path[id_counter] = tuple(p)\n",
    "                id_counter+=1\n",
    "        return each_flow_paths,each_path_id,each_id_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CFRRL_Game(Game):\n",
    "    def __init__(self, config, env,commitment_window,look_ahead_window,path_counter,avg_paths_per_t, random_seed=1000):\n",
    "        super(CFRRL_Game, self).__init__(config, env, random_seed)\n",
    "        \n",
    "        self.project_name = config.project_name\n",
    "        #env.num_pairs = self.num_pairs\n",
    "        self.action_dim = path_counter\n",
    "        #print(\"self.action_dim is %s path_counter is %s \"%(self.action_dim,path_counter))\n",
    "        self.max_moves = int(self.action_dim * (config.max_moves / 100.))\n",
    "        self.max_moves = avg_paths_per_t\n",
    "        #print(\"self.max_moves %s self.action_dim %s self.max_moves %s self.action_dim %s\"\n",
    "       #       %(self.max_moves , self.action_dim, self.max_moves, self.action_dim))\n",
    "        #assert self.max_moves <= self.action_dim, (self.max_moves, self.action_dim)\n",
    "        \n",
    "        self.tm_history = 1\n",
    "        self.tm_history=commitment_window\n",
    "        self.tm_indexes = np.arange(self.tm_history-1, self.tm_cnt)\n",
    "        self.valid_tm_cnt = len(self.tm_indexes)\n",
    "        \n",
    "        if config.method == 'pure_policy':\n",
    "            self.baseline = {}\n",
    "\n",
    "        self.generate_inputs(normalization=True)\n",
    "        self.state_dims = self.normalized_traffic_matrices.shape[1:]\n",
    "        print('Input dims :', self.state_dims)\n",
    "        print('Max moves :', self.max_moves)\n",
    "        import pdb\n",
    "        print('whole shapes ',self.normalized_traffic_matrices.shape)\n",
    "        print('avg_paths_per_t',avg_paths_per_t)\n",
    "        #pdb.set_trace()\n",
    "        \"\"\"two new variables\"\"\"\n",
    "        \n",
    "        self.commitment_window =commitment_window\n",
    "        self.look_ahead_window = look_ahead_window\n",
    "\n",
    "    def get_state(self, tm_idx):\n",
    "        idx_offset = self.tm_history - 1\n",
    "        return self.normalized_traffic_matrices[tm_idx-idx_offset]\n",
    "    def reward2(self, tm_idx,all_tms,look_ahead_window,each_flow_edges,each_flow_paths,each_path_id,each_id_path,each_flow_shortest_path,topology_file):\n",
    "        \n",
    "#         _, solution = self.optimal_routing_mlu(tm_idx)\n",
    "#         optimal_mlu, optimal_mlu_delay = self.eval_optimal_routing_mlu(tm_idx, solution, eval_delay=False)\n",
    "#         print(\"here is the optimal mlu\",optimal_mlu)\n",
    "#         _, solution = self.optimal_routing_mlu_default(tm_idx)\n",
    "#         optimal_mlu, optimal_mlu_delay = self.eval_optimal_routing_mlu(tm_idx, solution, eval_delay=False)\n",
    "#         print(\"here is the optimal mlu 2\",optimal_mlu)\n",
    "#         mlu, _ = self.optimal_routing_mlu_critical_pairs(tm_idx, actions)\n",
    "        \n",
    "#         print(\"here is the optimal mlu using critical flow rerouting\",optimal_mlu)\n",
    "        \n",
    "        drl_mlus,solutions = self.mlu_routing_selected_paths(tm_idx,look_ahead_window,each_flow_edges)\n",
    "        drl_mlu = sum(drl_mlus)/len(drl_mlus)\n",
    "        #print(\"this is the avg mlu using the suggested paths \",drl_mlu)\n",
    "        \n",
    "        #each_flow_edges_hindsight_approach = self.get_hindsight_set_of_paths(tm_idx,look_ahead_window,each_flow_paths,each_path_id,each_id_path,each_flow_shortest_path,topology_file)\n",
    "        #hindsight_mlu = self.mlu_routing_selected_paths(tm_idx,look_ahead_window,each_flow_edges_hindsight_approach)\n",
    "        mlu_optimal_mlus,mlu_optimal_solutions = self.mlu_optimal_routing_mlu(tm_idx,look_ahead_window)\n",
    "        mlu_optimal_mlu = sum(mlu_optimal_mlus)/len(mlu_optimal_mlus)\n",
    "        reward = mlu_optimal_mlu/drl_mlu\n",
    "        print(\"for tm_idx point %s from %s rl is %s mlu optimal approach is %s and the reward is %s\"%(tm_idx,all_tms,drl_mlu,mlu_optimal_mlu,reward))\n",
    "        return reward\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        optimal_mlu = self.get_optimal_avg_mlu_hindsight(tm_idx,look_ahead_window)\n",
    "        \n",
    "    def reward(self, tm_idx, actions):\n",
    "        mlu, _ = self.optimal_routing_mlu_critical_pairs(tm_idx, actions)\n",
    "\n",
    "        reward = 1 / mlu\n",
    "\n",
    "        return reward\n",
    "    def get_all_trainig_epochs(self,commitment_window,look_ahead_window):\n",
    "        indx = 1\n",
    "        epochs = []\n",
    "        epochs.append(1)\n",
    "        epochs.append(9)\n",
    "        while(max(epochs)<319):\n",
    "            epochs.append(max(epochs)+10)\n",
    "        \n",
    "        \n",
    "        return list(epochs)\n",
    "    def advantage(self, tm_idx, reward):\n",
    "        if tm_idx not in self.baseline:\n",
    "            return reward\n",
    "\n",
    "        total_v, cnt = self.baseline[tm_idx]\n",
    "        \n",
    "        #print(reward, (total_v/cnt))\n",
    "\n",
    "        return reward - (total_v/cnt)\n",
    "\n",
    "    def update_baseline(self, tm_idx, reward):\n",
    "        if tm_idx in self.baseline:\n",
    "            total_v, cnt = self.baseline[tm_idx]\n",
    "\n",
    "            total_v += reward\n",
    "            cnt += 1\n",
    "\n",
    "            self.baseline[tm_idx] = (total_v, cnt)\n",
    "        else:\n",
    "            self.baseline[tm_idx] = (reward, 1)\n",
    "\n",
    "    def evaluate(self, tm_idx, actions=None, ecmp=True, eval_delay=False):\n",
    "        \n",
    "        _, solution = self.optimal_routing_mlu(tm_idx)\n",
    "        optimal_mlu, optimal_mlu_delay = self.eval_optimal_routing_mlu(tm_idx, solution, eval_delay=eval_delay)\n",
    "        #print(\"for tm_idx %s we have mlu %s\"%(tm_idx,optimal_mlu))\n",
    "        if ecmp:\n",
    "            ecmp_mlu, ecmp_delay = self.eval_ecmp_traffic_distribution(tm_idx, eval_delay=eval_delay)\n",
    "        \n",
    "        _, solution = self.optimal_routing_mlu_critical_pairs(tm_idx, actions)\n",
    "        mlu, delay = self.eval_critical_flow_and_ecmp(tm_idx, actions, solution, eval_delay=eval_delay)\n",
    "\n",
    "        crit_topk = self.get_critical_topK_flows(tm_idx)\n",
    "        _, solution = self.optimal_routing_mlu_critical_pairs(tm_idx, crit_topk)\n",
    "        crit_mlu, crit_delay = self.eval_critical_flow_and_ecmp(tm_idx, crit_topk, solution, eval_delay=eval_delay)\n",
    "\n",
    "        topk = self.get_topK_flows(tm_idx, self.lp_pairs)\n",
    "        _, solution = self.optimal_routing_mlu_critical_pairs(tm_idx, topk)\n",
    "        topk_mlu, topk_delay = self.eval_critical_flow_and_ecmp(tm_idx, topk, solution, eval_delay=eval_delay)\n",
    "\n",
    "\n",
    "        norm_mlu = optimal_mlu / mlu\n",
    "        line = str(tm_idx) + ', ' + str(norm_mlu) + ', ' + str(mlu) + ', ' \n",
    "        \n",
    "        norm_crit_mlu = optimal_mlu / crit_mlu\n",
    "        line += str(norm_crit_mlu) + ', ' + str(crit_mlu) + ', ' \n",
    "\n",
    "        norm_topk_mlu = optimal_mlu / topk_mlu\n",
    "        line += str(norm_topk_mlu) + ', ' + str(topk_mlu) + ', ' \n",
    "\n",
    "        if ecmp:\n",
    "            norm_ecmp_mlu = optimal_mlu / ecmp_mlu\n",
    "            line += str(norm_ecmp_mlu) + ', ' + str(ecmp_mlu) + ', '\n",
    "\n",
    "        if eval_delay:\n",
    "            solution = self.optimal_routing_delay(tm_idx)\n",
    "            optimal_delay = self.eval_optimal_routing_delay(tm_idx, solution) \n",
    "\n",
    "            line += str(optimal_delay/delay) + ', ' \n",
    "            line += str(optimal_delay/crit_delay) + ', ' \n",
    "            line += str(optimal_delay/topk_delay) + ', ' \n",
    "            line += str(optimal_delay/optimal_mlu_delay) + ', '\n",
    "            if ecmp:\n",
    "                line += str(optimal_delay/ecmp_delay) + ', '\n",
    "        \n",
    "            assert tm_idx in self.load_multiplier, (tm_idx)\n",
    "            line += str(self.load_multiplier[tm_idx]) + ', '\n",
    "\n",
    "        print(line[:-2])\n",
    "    def evaluate2(self, env,config,tm_idx,all_tms,training_epoch,commitment_window, look_ahead_window,topology_file,actions=None, ecmp=True, eval_delay=False):\n",
    "        toplogy_t_solution_mlu_result = config.testing_results\n",
    "        time_index = tm_idx\n",
    "        each_flow_shortest_path = env.topology.get_each_flow_shortest_paths()\n",
    "        each_flow_paths,each_path_id,each_id_path = self.get_each_flow_paths_and_path_id(topology_file)\n",
    "        each_flow_edges_hindsight_approach = self.get_hindsight_set_of_paths(tm_idx,\n",
    "                                                                             look_ahead_window,each_flow_paths,\n",
    "                                                                             each_path_id,each_id_path,\n",
    "                                                                             each_flow_shortest_path,topology_file,config)\n",
    "        \n",
    "        hindsight_mlus,hindsight_solutions = self.mlu_routing_selected_paths(tm_idx,look_ahead_window,\n",
    "                                                                             each_flow_edges_hindsight_approach)\n",
    "        \n",
    "        \n",
    "        \"\"\"for ECMP mlu\"\"\"\n",
    "        time_index = tm_idx\n",
    "        ecmp_mlus = self.eval_ecmp_traffic_distribution2(tm_idx,look_ahead_window, eval_delay=eval_delay)\n",
    "        \n",
    "        \"\"\"for Oblivious routing\"\"\"\n",
    "        time_index = tm_idx\n",
    "        max_move = len(actions)\n",
    "        each_flow_oblivious_paths_edges = env.topology.get_oblivious_paths_each_flow_edges(config.raeke_paths,\n",
    "                                                                                          max_move,each_flow_shortest_path)\n",
    "        #print(\"oblivious edge flows\",len(list(each_flow_oblivious_paths_edges.keys())))\n",
    "        #print(\"shortest_paths\",len(list(each_flow_shortest_path.keys())))\n",
    "#         for flow,edges in each_flow_oblivious_paths_edges.items():\n",
    "#             print(\"flow %s has edges %s\"%(flow,edges))\n",
    "        oblivious_mlus,oblivious_solutions = self.mlu_routing_selected_paths_oblivious(tm_idx,look_ahead_window,\n",
    "                                                                             each_flow_oblivious_paths_edges)\n",
    "        \n",
    "        \n",
    "      \n",
    "        \"\"\"for MLU_optimal\"\"\"\n",
    "        time_index = tm_idx\n",
    "        mlu_optimal_mlus,mlu_optimal_solutions = self.mlu_optimal_routing_mlu(tm_idx,look_ahead_window)\n",
    "        \n",
    "       \n",
    "        \"\"\"for RL approach\"\"\"\n",
    "        time_index = tm_idx\n",
    "        \n",
    "        \n",
    "        each_flow_edges_rl_approach = self.get_suggested_paths_by_rl(actions,each_flow_paths,\n",
    "                                                                     each_path_id,each_id_path,\n",
    "                                                                     each_flow_shortest_path)\n",
    "        rl_mlus,rl_solutions = self.mlu_routing_selected_paths(tm_idx,look_ahead_window,\n",
    "                                                               each_flow_edges_rl_approach)\n",
    "        mlu_index = 0 \n",
    "        for sol in rl_solutions:\n",
    "            print(\"c:\",commitment_window,\"w:\",look_ahead_window,\"epoch:\",training_epoch,\"t:\",time_index,\"all_tms:\",all_tms,\n",
    "                        \"optimal\",round(hindsight_mlus[mlu_index],3),\n",
    "                        \"mlu_optimal\",\n",
    "                        round(mlu_optimal_mlus[mlu_index],3),\n",
    "                        \"rl\",\n",
    "                        round(rl_mlus[mlu_index],3),\n",
    "                        \"ecmp\",round(ecmp_mlus[mlu_index],3),\n",
    "                        \"oblivious\",round(oblivious_mlus[mlu_index],3))\n",
    "            if round(oblivious_mlus[mlu_index],3) <round(mlu_optimal_mlus[mlu_index],3):\n",
    "                print(\"this does not make sense!!!!!!!\")\n",
    "            mlu_index+=1\n",
    "        mlu_index = 0\n",
    "        with open(toplogy_t_solution_mlu_result, 'a') as newFile:                                \n",
    "            newFileWriter = csv.writer(newFile)\n",
    "            solution_indx = 0\n",
    "            for solution in hindsight_solutions:\n",
    "                for item,v in solution.items():\n",
    "                    #print('flow index %s from node %s to the next node %s ratio %s'%(item[0],item[1],item[2],v))\n",
    "                    newFileWriter.writerow([topology_file,commitment_window,look_ahead_window,time_index,\n",
    "                    \"optimal\",item[0],item[1],item[2],v,round(hindsight_mlus[mlu_index],3),\n",
    "                    \"mlu_optimal\",item[0],item[1],item[2],\n",
    "                    mlu_optimal_solutions[solution_indx][(item[0],item[1],item[2])],\n",
    "                    round(mlu_optimal_mlus[mlu_index],3),\n",
    "                    \"rl\",item[0],item[1],item[2],\n",
    "                    rl_solutions[solution_indx][(item[0],item[1],item[2])],\n",
    "                    round(rl_mlus[mlu_index],3),\n",
    "                    \"ecmp\",round(ecmp_mlus[mlu_index],3),\n",
    "                    \"oblivious\",round(oblivious_mlus[mlu_index],3),training_epoch])\n",
    "                time_index+=1\n",
    "                mlu_index+=1\n",
    "                solution_indx+=1\n",
    "                \n",
    "                \n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
