{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow as tf\n",
    "from env import Environment\n",
    "from game import CFRRL_Game\n",
    "from model import Network\n",
    "from config import get_config\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('ckpt', '', 'apply a specific checkpoint')\n",
    "flags.DEFINE_boolean('eval_delay', False, 'evaluate delay or not')\n",
    "def get_paths_number(topology_name,each_topology_each_t_each_f_paths):\n",
    "    \n",
    "    \n",
    "    all_the_paths = set([])\n",
    "    each_t_paths = {}\n",
    "    #print('topology is ',topology_name)\n",
    "    with open(each_topology_each_t_each_f_paths) as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            if topology_name in line:#ATT_topology_file_modified:189:1->20: ['1', '13', '9', '4', '20'],['1', '9', '4', '20']\n",
    "                time = line.split(\":\")[1]\n",
    "                flow_str = line.split(\":\")[2]\n",
    "                flow = flow_str.split(\"->\")\n",
    "                flow = (int(flow[0]),int(flow[1]))\n",
    "                paths = line.split(flow_str+\":\")[1]\n",
    "                paths = paths.strip()\n",
    "                paths = \"\\\"\"+str(paths)+\"\\\"\"\n",
    "                paths = ast.literal_eval(paths)\n",
    "                #print('time %s flow %s paths %s'%(time,flow, paths))\n",
    "                #print(type(paths),len(paths))\n",
    "                if '],[' in paths:\n",
    "                    paths = ast.literal_eval(paths)\n",
    "                else:\n",
    "                    paths = ast.literal_eval(paths)\n",
    "                    new_path = []\n",
    "                    for p in paths:\n",
    "                        new_path.append(p)\n",
    "                    paths = [new_path]\n",
    "                #print('2time %s flow %s paths %s'%(time,flow, paths))\n",
    "                #print(type(paths),len(paths))\n",
    "                for p in paths:\n",
    "                    p = tuple(p)\n",
    "                    new_p= []\n",
    "                    for node in p:\n",
    "                        new_p.append(int(node))\n",
    "                    p = tuple(new_p)\n",
    "                    \n",
    "                    try:\n",
    "                        each_t_paths[time].add(p)\n",
    "                    except:\n",
    "                        each_t_paths[time] = set([p])\n",
    "                    all_the_paths.add(p)\n",
    "                    \n",
    "    paths_number = []\n",
    "    for time, paths in each_t_paths.items():\n",
    "        #print('for time %s we had %s paths'%(time,len(paths)))\n",
    "        paths_number.append(len(paths))\n",
    "    avg_paths_per_time = int((sum(paths_number)/len(paths_number)))\n",
    "    return len(list(all_the_paths)),avg_paths_per_time\n",
    "def sim(config, network,env, game,commitment_window,look_ahead_window,training_epoch):\n",
    "    counter = 0\n",
    "    for tm_idx in range(commitment_window,len(game.tm_indexes)-look_ahead_window-1):\n",
    "#        print(\"tm_idx is \" ,tm_idx)\n",
    "        counter+=1\n",
    "        #print(\"done %s from %s traffic matrices for commitment %s lookahead %s\"%(counter,len(game.tm_indexes),commitment_window,look_ahead_window))\n",
    "        state = game.get_state(tm_idx)\n",
    "        if config.method == 'actor_critic':\n",
    "            policy = network.actor_predict(np.expand_dims(state, 0)).numpy()[0]\n",
    "        elif config.method == 'pure_policy':\n",
    "            policy = network.policy_predict(np.expand_dims(state, 0)).numpy()[0]\n",
    "        actions = policy.argsort()[-game.max_moves:]\n",
    "\n",
    "        game.evaluate2(env,config,tm_idx,len(game.tm_indexes),training_epoch,commitment_window,look_ahead_window,config.topology_file, actions,training_epoch, eval_delay=False) \n",
    "\n",
    "def main(_):\n",
    "    #Using cpu for testing\n",
    "    tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "    tf.get_logger().setLevel('INFO')\n",
    "\n",
    "    config = get_config(FLAGS) or FLAGS\n",
    "    env = Environment(config, is_training=False)\n",
    "    \n",
    "    each_flow_shortest_paths = env.topology.get_each_flow_shortest_paths()\n",
    "    path_counter,avg_paths_per_time = get_paths_number(config.topology_file,config.each_topology_each_t_each_f_paths)\n",
    "    for commitment_window in range(2,int(config.commitment_window_range)):\n",
    "        for look_ahead_window in range(2,int(config.look_ahead_window_range)):\n",
    "            \"\"\"we first find the candidate paths and use it for action dimention\"\"\"\n",
    "            game = CFRRL_Game(config, env,commitment_window,look_ahead_window,path_counter,avg_paths_per_time)\n",
    "            training_epochs = game.get_all_trainig_epochs(commitment_window,look_ahead_window)\n",
    "            #training_epochs = [1,9,19,29,39,49]\n",
    "            \n",
    "            if not config.training_epochs_experiment:\n",
    "                training_epochs = [max(training_epochs)]\n",
    "            print(\"training_epochs\",training_epochs)\n",
    "            for training_epoch in training_epochs:\n",
    "                \n",
    "                \"\"\"we modify and set the max move to the avg used path at each t for  all flows\"\"\"\n",
    "                game.max_moves = avg_paths_per_time\n",
    "\n",
    "                network = Network(config, game.state_dims, game.action_dim, game.max_moves,commitment_window,look_ahead_window,training_epoch)\n",
    "                print(\"This is the chkpoint path \",network.ckpt_dir)\n",
    "                step = network.restore_ckpt(FLAGS.ckpt)\n",
    "                if config.method == 'actor_critic':\n",
    "                    learning_rate = network.lr_schedule(network.actor_optimizer.iterations.numpy()).numpy()\n",
    "                elif config.method == 'pure_policy':\n",
    "                    learning_rate = network.lr_schedule(network.optimizer.iterations.numpy()).numpy()\n",
    "                print('\\nstep %d, learning rate: %f\\n'% (step, learning_rate))\n",
    "\n",
    "                sim(config, network,env, game,commitment_window,look_ahead_window,training_epoch)\n",
    "if __name__ == '__main__':\n",
    "    app.run(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
