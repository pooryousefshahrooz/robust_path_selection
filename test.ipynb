{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a491e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow as tf\n",
    "from env import Environment\n",
    "from game import CFRRL_Game\n",
    "from model import Network\n",
    "from config import get_config\n",
    "import csv\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('ckpt', '', 'apply a specific checkpoint')\n",
    "flags.DEFINE_boolean('eval_delay', False, 'evaluate delay or not')\n",
    "\n",
    "\n",
    "def get_other_schemes_mlu(scheme):\n",
    "    \n",
    "    return mlu_greedy_solutions,\n",
    "def sim(config, network,tm_idx,env, game,commitment_window,look_ahead_window,scheme):\n",
    "    counter = 0\n",
    "#        print(\"tm_idx is \" ,tm_idx)\n",
    "    counter+=1\n",
    "    #print(\"done %s from %s traffic matrices for commitment %s lookahead %s\"%(counter,len(game.tm_indexes),commitment_window,look_ahead_window))\n",
    "    state = game.get_state(tm_idx)\n",
    "    if config.method == 'actor_critic':\n",
    "        policy = network.actor_predict(np.expand_dims(state, 0)).numpy()[0]\n",
    "    elif config.method == 'pure_policy':\n",
    "        policy = network.policy_predict(np.expand_dims(state, 0)).numpy()[0]\n",
    "    actions = policy.argsort()[-game.max_moves:]\n",
    "    max_move = game.max_moves\n",
    "    #game.evaluate3(env,config,tm_idx,len(game.tm_indexes),training_epoch,commitment_window,look_ahead_window,config.topology_file, actions,training_epoch, eval_delay=False) \n",
    "    drl_mlus,drl_solutions,_,_ = game.evaluate2(env,config,tm_idx,len(game.tm_indexes),commitment_window,look_ahead_window,config.topology_file,scheme, max_move,actions, eval_delay=False) \n",
    "    solutions2 = {}\n",
    "    return drl_mlus,drl_solutions,1,solutions2\n",
    "def main(_):\n",
    "    #Using cpu for testing\n",
    "    tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "    tf.get_logger().setLevel('INFO')\n",
    "\n",
    "    config = get_config(FLAGS) or FLAGS\n",
    "    env = Environment(config, is_training=False)\n",
    "    \n",
    "    each_flow_shortest_paths = env.topology.get_each_flow_shortest_paths()\n",
    "    path_counter,avg_paths_per_time = game.get_paths_info(config.topology_file,config.each_topology_each_t_each_f_paths)\n",
    "    #for commitment_window in range(2,int(config.commitment_window_range)):\n",
    "    for commitment_window in [4,6,2,8,10]:\n",
    "        #for look_ahead_window in range(6,int(config.look_ahead_window_range)):\n",
    "        for look_ahead_window in [2,4,6,8,10]:\n",
    "            \"\"\"we first find the candidate paths and use it for action dimention\"\"\"\n",
    "            game = CFRRL_Game(config, env,commitment_window,look_ahead_window,path_counter,avg_paths_per_time)\n",
    "            game.max_moves = avg_paths_per_time\n",
    "            max_value = avg_paths_per_time\n",
    "            training_epochs = game.get_all_trainig_epochs(commitment_window,look_ahead_window)\n",
    "            #training_epochs = [1,9,19,29,39,49]\n",
    "            actions = []\n",
    "            all_tms = len(game.tm_indexes)\n",
    "            if not config.training_epochs_experiment:\n",
    "                training_epochs = [max(training_epochs)]\n",
    "            for training_epoch in training_epochs:\n",
    "                network = Network(config, game.state_dims, game.action_dim, game.max_moves,commitment_window,look_ahead_window,training_epoch)\n",
    "                #print(\"This is the chkpoint path \",network.ckpt_dir)\n",
    "                step = network.restore_ckpt(FLAGS.ckpt)\n",
    "                if config.method == 'actor_critic':\n",
    "                    learning_rate = network.lr_schedule(network.actor_optimizer.iterations.numpy()).numpy()\n",
    "                elif config.method == 'pure_policy':\n",
    "                    learning_rate = network.lr_schedule(network.optimizer.iterations.numpy()).numpy()\n",
    "                print('\\nstep %d, learning rate: %f\\n'% (step, learning_rate))\n",
    "                    \n",
    "                for tm_idx in range(commitment_window,len(game.tm_indexes)-look_ahead_window-1):\n",
    "                    if training_epoch==min(training_epochs):\n",
    "                        mlu_greedy_mlus,mlu_greedy_solutions,_,_ = game.evaluate2(env,config,tm_idx,len(game.tm_indexes),commitment_window,look_ahead_window,config.topology_file,\"MLU-greedy\", max_value,actions, eval_delay=False)\n",
    "                        ecmp_mlus,_,_,_ = game.evaluate2(env,config,tm_idx,len(game.tm_indexes),commitment_window,look_ahead_window,config.topology_file, \"ECMP\",max_value,actions, eval_delay=False)\n",
    "#                         oblivious_mlus,_,_,_ = game.evaluate2(env,config,tm_idx,len(game.tm_indexes),commitment_window,look_ahead_window,config.topology_file,\"Oblivious\", max_value,actions, eval_delay=False)\n",
    "#                         oblivious2_mlus,_,_,_ = game.evaluate2(env,config,tm_idx,len(game.tm_indexes),commitment_window,look_ahead_window,config.topology_file,\"Oblivious2\", max_value,actions, eval_delay=False)\n",
    "                        optimal1_mlus,_,optimal2_mlus,_ = game.evaluate2(env,config,tm_idx,len(game.tm_indexes),commitment_window,look_ahead_window,config.topology_file,\"Optimal\", max_value,actions, eval_delay=False)\n",
    "                    toplogy_t_solution_mlu_result = config.testing_results\n",
    "                    #print(\"training_epochs\",training_epochs)\n",
    "                \n",
    "                    \"\"\"we modify and set the max move to the avg used path at each t for  all flows\"\"\"\n",
    "                    \n",
    "                    rl_mlus,_,_,_= sim(config, network,tm_idx,env, game,commitment_window,look_ahead_window,\"DRL\")\n",
    "                    mlu_index = 0\n",
    "                    time_index = tm_idx\n",
    "                    mlu_index = 0 \n",
    "                    for sol in mlu_greedy_solutions:\n",
    "                        print(\"c:\",commitment_window,\"w:\",look_ahead_window,\n",
    "                              \"epoch:\",training_epoch,\"t:\",time_index,\n",
    "                              \"all_tms:\",all_tms,\n",
    "                                    \"optimal\",round(optimal1_mlus[mlu_index],3),\n",
    "                                    \"mlu_greedy\",\n",
    "                                    round(mlu_greedy_mlus[mlu_index],3),\n",
    "                                    \"rl\",\n",
    "                                    round(rl_mlus[mlu_index],3),\n",
    "                                    \"ecmp\",round(ecmp_mlus[mlu_index],3),\n",
    "                                    \"oblivious\",round(oblivious_mlus[mlu_index],3),\n",
    "                                      \"oblivious2\",round(oblivious2_mlus[mlu_index],3))\n",
    "                        if round(oblivious_mlus[mlu_index],3) <round(mlu_greedy_mlus[mlu_index],3):\n",
    "                            print(\"this does not make sense!!!!!!!\")\n",
    "                        mlu_index+=1\n",
    "                    mlu_index = 0\n",
    "                    with open(toplogy_t_solution_mlu_result, 'a') as newFile:                                \n",
    "                        newFileWriter = csv.writer(newFile)\n",
    "                        solution_indx = 0\n",
    "                        for solution in mlu_greedy_solutions:\n",
    "                            for item,v in solution.items():\n",
    "                                #print('flow index %s from node %s to the next node %s ratio %s'%(item[0],item[1],item[2],v))\n",
    "                                newFileWriter.writerow([config.topology_file,commitment_window,\n",
    "                                look_ahead_window,time_index,\n",
    "                                \"optimal\",round(optimal1_mlus[mlu_index],3),\n",
    "                                \"mlu_greedy\",item[0],item[1],item[2],\n",
    "                                mlu_greedy_solutions[solution_indx][(item[0],item[1],item[2])],\n",
    "                                round(mlu_greedy_mlus[mlu_index],3),\n",
    "                                \"rl\",\n",
    "                                round(rl_mlus[mlu_index],3),\n",
    "                                \"ecmp\",round(ecmp_mlus[mlu_index],3),\n",
    "                                \"oblivious\",round(oblivious_mlus[mlu_index],3),\n",
    "                                \"oblivious2\",round(oblivious2_mlus[mlu_index],3),training_epoch])\n",
    "                            time_index+=1\n",
    "                            mlu_index+=1\n",
    "                            solution_indx+=1\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    app.run(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
