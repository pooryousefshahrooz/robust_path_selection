{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc99fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from env import Environment\n",
    "from game import CFRRL_Game\n",
    "from model import Network\n",
    "from config import get_config\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('num_agents',1, 'number of agents')\n",
    "flags.DEFINE_string('baseline', 'avg', 'avg: use average reward as baseline, best: best reward as baseline')\n",
    "flags.DEFINE_integer('num_iter', 20, 'Number of iterations each agent would run')\n",
    "# print(FLAGS.num_agents)\n",
    "import pdb\n",
    "# pdb.set_trace()\n",
    "GRADIENTS_CHECK=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560282a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paths(topology_name,each_topology_each_t_each_f_paths):\n",
    "    \"\"\"in this function, we get the B=4 most used paths by each flow\n",
    "    \n",
    "    we get all the used path by each flow over all times and get top 4 of them\"\"\"\n",
    "    path_counter = 0\n",
    "    each_path_id = {}\n",
    "    set_of_times = set([])\n",
    "    each_flow_path_usage = {}\n",
    "    all_the_paths = set([])\n",
    "    each_t_paths = {}\n",
    "    each_flow_paths = {}\n",
    "    \n",
    "    #print('topology is ',topology_name)\n",
    "    with open(each_topology_each_t_each_f_paths) as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            if topology_name in line:#ATT_topology_file_modified:189:1->20: ['1', '13', '9', '4', '20'],['1', '9', '4', '20']\n",
    "                time = line.split(\":\")[1]\n",
    "                flow_str = line.split(\":\")[2]\n",
    "                flow = flow_str.split(\"->\")\n",
    "                flow = (int(flow[0]),int(flow[1]))\n",
    "                set_of_times.add(time)\n",
    "                paths = line.split(flow_str+\":\")[1]\n",
    "                paths = paths.strip()\n",
    "                paths = \"\\\"\"+str(paths)+\"\\\"\"\n",
    "                paths = ast.literal_eval(paths)\n",
    "                #print('time %s flow %s paths %s'%(time,flow, paths))\n",
    "                #print(type(paths),len(paths))\n",
    "                if '],[' in paths:\n",
    "                    paths = ast.literal_eval(paths)\n",
    "                else:\n",
    "                    paths = ast.literal_eval(paths)\n",
    "                    new_path = []\n",
    "                    for p in paths:\n",
    "                        new_path.append(p)\n",
    "                    paths = [new_path]\n",
    "                #print('2time %s flow %s paths %s'%(time,flow, paths))\n",
    "                #print(type(paths),len(paths))\n",
    "                for p in paths:\n",
    "                    p = tuple(p)\n",
    "                    new_p= []\n",
    "                    for node in p:\n",
    "                        new_p.append(int(node))\n",
    "                    p = tuple(new_p)\n",
    "                    try:\n",
    "                        each_flow_paths[flow].add(p)\n",
    "                    except:\n",
    "                        each_flow_paths[flow] =set([p])\n",
    "                    try:\n",
    "                        each_t_paths[time].add(p)\n",
    "                    except:\n",
    "                        each_t_paths[time] = set([p])\n",
    "                    all_the_paths.add(p)\n",
    "                    #print('this is a path ',p)\n",
    "                    try:\n",
    "                        each_flow_path_usage[flow][p]+=1\n",
    "                    except:\n",
    "                        try:\n",
    "                            each_flow_path_usage[flow][p]= 1\n",
    "                        except:\n",
    "                            each_flow_path_usage[flow]= {}\n",
    "                            each_flow_path_usage[flow][p]= 1\n",
    "                \n",
    "                \n",
    "    each_flow_top_four_used_paths = {}\n",
    "    for each_flow,path_usage in each_flow_path_usage.items():\n",
    "        \n",
    "        top_used_numbers = []\n",
    "        for p, usage in path_usage.items():\n",
    "            #print(\"each flow % path  used %s \"%(each_flow,usage))\n",
    "            top_used_numbers.append(usage)\n",
    "        \n",
    "        top_used_numbers.sort()\n",
    "        #print('flow %s uses %s paths '%(each_flow,len(top_used_numbers)))\n",
    "        top_used_numbers = top_used_numbers[-1:]\n",
    "#         if len(top_used_numbers) >1:\n",
    "#             top_used_numbers = top_used_numbers[-5:]\n",
    "#         else:\n",
    "#             if len(top_used_numbers)==4:\n",
    "#                 top_used_numbers.append(top_used_numbers[2])\n",
    "#             elif len(top_used_numbers)==3:\n",
    "#                 top_used_numbers.append(top_used_numbers[1])\n",
    "#                 top_used_numbers.append(top_used_numbers[1])\n",
    "#             elif len(top_used_numbers)==1:\n",
    "#                 top_used_numbers.append(top_used_numbers[0])\n",
    "#                 top_used_numbers.append(top_used_numbers[0])\n",
    "#                 top_used_numbers.append(top_used_numbers[0])\n",
    "#                 top_used_numbers.append(top_used_numbers[0])\n",
    "#             top_used_numbers = top_used_numbers\n",
    "        each_flow_top_four_used_paths[each_flow] = top_used_numbers\n",
    "        \n",
    "    each_flow_top_used_paths = {}\n",
    "    for each_flow,top_used_numbers in each_flow_top_four_used_paths.items():\n",
    "        for flow,paths in each_flow_path_usage.items():\n",
    "            if each_flow ==flow:\n",
    "                for p, usage_times in paths.items():\n",
    "                    if usage_times in top_used_numbers:\n",
    "                        try:\n",
    "                            if p not in each_flow_top_used_paths[flow]:\n",
    "                                each_flow_top_used_paths[flow].append(p)\n",
    "                        except:\n",
    "                            each_flow_top_used_paths[flow]= [p]\n",
    "    import pdb\n",
    "#     for flow, top_paths in each_flow_top_used_paths.items():\n",
    "#         print(\"flow %s paths %s\"%(flow,top_paths))\n",
    "        \n",
    "    each_flow_most_used_paths = {}\n",
    "    for flow, paths in each_flow_top_used_paths.items():\n",
    "        new_paths = []\n",
    "        for p in paths:\n",
    "            new_paths.append(p)\n",
    "#         if len(new_paths)==3:\n",
    "#             new_paths.append(p)\n",
    "#         elif len(new_paths)==2:\n",
    "#             new_paths.append(p)\n",
    "#             new_paths.append(p)\n",
    "#         elif len(new_paths)==1:\n",
    "#             new_paths.append(p)\n",
    "#             new_paths.append(p)\n",
    "#             new_paths.append(p)\n",
    "        each_flow_most_used_paths[flow] = new_paths\n",
    "    for flow, top_paths in each_flow_most_used_paths.items():\n",
    "        path_counter+=len(top_paths)\n",
    "        #print(\"flow %s paths %s\"%(flow,len(top_paths)))\n",
    "    import pdb\n",
    "    #print('al the paths ',len(list(all_the_paths)))\n",
    "    paths_number = []\n",
    "    for time, paths in each_t_paths.items():\n",
    "        #print('for time %s we had %s paths'%(time,len(paths)))\n",
    "        paths_number.append(len(paths))\n",
    "    avg_paths_per_time = int((sum(paths_number)/len(paths_number)))\n",
    "    #print(\"avg path used per time slot %s \"%(avg_paths_per_time))\n",
    "    #pdb.set_trace()\n",
    "    return len(list(all_the_paths)),avg_paths_per_time,each_flow_paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5866ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_agent(config, game,commitment_window,look_ahead_window, model_weights_queues, experience_queues):\n",
    "    network = Network(config, game.state_dims, game.action_dim, game.max_moves,commitment_window,look_ahead_window,1, master=True)\n",
    "    print(\"config.max_step\",config.max_step)\n",
    "    import pdb\n",
    "    #pdb.set_trace()\n",
    "    network.save_hyperparams(config)\n",
    "    start_step = network.restore_ckpt()\n",
    "    number_of_training_epochs = 0\n",
    "    for step in tqdm(range(start_step, config.max_step), ncols=70, initial=start_step):\n",
    "        network.ckpt.step.assign_add(1)\n",
    "        model_weights = network.model.get_weights()\n",
    "        number_of_training_epochs+=1\n",
    "#         print(\"***********************\")\n",
    "#         print(len(model_weights_queues))\n",
    "#         print(FLAGS.num_agents)\n",
    "#         print(\"************.....**********\")\n",
    "#         import pdb\n",
    "        #pdb.set_tracce()\n",
    "        for i in range(FLAGS.num_agents):\n",
    "            model_weights_queues[i].put(model_weights)\n",
    "\n",
    "        if config.method == 'actor_critic':\n",
    "            #assemble experiences from the agents\n",
    "            s_batch = []\n",
    "            a_batch = []\n",
    "            r_batch = []\n",
    "\n",
    "            for i in range(FLAGS.num_agents):\n",
    "                s_batch_agent, a_batch_agent, r_batch_agent = experience_queues[i].get()\n",
    "              \n",
    "                assert len(s_batch_agent) == FLAGS.num_iter, \\\n",
    "                    (len(s_batch_agent), len(a_batch_agent), len(r_batch_agent))\n",
    "\n",
    "                s_batch += s_batch_agent\n",
    "                a_batch += a_batch_agent\n",
    "                r_batch += r_batch_agent\n",
    "           \n",
    "            assert len(s_batch)*game.max_moves == len(a_batch)\n",
    "            #used shared RMSProp, i.e., shared g\n",
    "            actions = np.eye(game.action_dim, dtype=np.float32)[np.array(a_batch)]\n",
    "            #print('this is game.action_dim',game.action_dim)\n",
    "            #print('this is a_batch',a_batch)\n",
    "#            print('this is actions',actions)\n",
    "            \n",
    "#             for item in actions:\n",
    "#                 print('len of each item in actions',len(item),item)\n",
    "            import pdb\n",
    "            #print('len of a_batch',len(a_batch),'len of actions',len(actions))\n",
    "            #print('this is the item in array',np.array(a_batch))\n",
    "            \n",
    "            #print(len(np.eye(game.action_dim, dtype=np.float32)),len(np.array(a_batch)))\n",
    "            #print(\"****\")\n",
    "#             print(a_batch)\n",
    "#             print(len(a_batch))\n",
    "#             print(actions)\n",
    "#             print(len(actions))\n",
    "#             print(len(np.array(a_batch)))\n",
    "#             print(len(np.array([0,1,2])))\n",
    "#             print(\"***********\")\n",
    "#             print('number of actions',len(actions))\n",
    "            #pdb.set_trace()\n",
    "            \n",
    "            value_loss, entropy, actor_gradients, critic_gradients = network.actor_critic_train(np.array(s_batch), \n",
    "                                                                    actions, \n",
    "                                                                    np.array(r_batch).astype(np.float32), \n",
    "                                                                    config.entropy_weight)\n",
    "       \n",
    "            if GRADIENTS_CHECK:\n",
    "                for g in range(len(actor_gradients)):\n",
    "                    assert np.any(np.isnan(actor_gradients[g])) == False, ('actor_gradients', s_batch, a_batch, r_batch, entropy)\n",
    "                for g in range(len(critic_gradients)):\n",
    "                    assert np.any(np.isnan(critic_gradients[g])) == False, ('critic_gradients', s_batch, a_batch, r_batch)\n",
    "\n",
    "            if step % config.save_step == config.save_step - 1:\n",
    "                print(\"check point dir befor update out is \",network.ckpt_dir)\n",
    "                network.update_chkpt_saving_dir(config,commitment_window,look_ahead_window,step)\n",
    "                print(\"check point dir now out is \",network.ckpt_dir)\n",
    "                network.save_ckpt(_print=True)\n",
    "                \n",
    "                #log training information\n",
    "                actor_learning_rate = network.lr_schedule(network.actor_optimizer.iterations.numpy()).numpy()\n",
    "                avg_value_loss = np.mean(value_loss)\n",
    "                avg_reward = np.mean(r_batch)\n",
    "                avg_entropy = np.mean(entropy)\n",
    "            \n",
    "                network.inject_summaries({\n",
    "                    'learning rate': actor_learning_rate,\n",
    "                    'value loss': avg_value_loss,\n",
    "                    'avg reward': avg_reward,\n",
    "                    'avg entropy': avg_entropy\n",
    "                    }, step)\n",
    "                print('lr:%f, value loss:%f, avg reward:%f, avg entropy:%f'%(actor_learning_rate, avg_value_loss, avg_reward, avg_entropy))\n",
    "\n",
    "        elif config.method == 'pure_policy':\n",
    "            #assemble experiences from the agents\n",
    "            s_batch = []\n",
    "            a_batch = []\n",
    "            r_batch = []\n",
    "            ad_batch = []\n",
    "\n",
    "            for i in range(FLAGS.num_agents):\n",
    "                s_batch_agent, a_batch_agent, r_batch_agent, ad_batch_agent = experience_queues[i].get()\n",
    "              \n",
    "                assert len(s_batch_agent) == FLAGS.num_iter, \\\n",
    "                    (len(s_batch_agent), len(a_batch_agent), len(r_batch_agent), len(ad_batch_agent))\n",
    "\n",
    "                s_batch += s_batch_agent\n",
    "                a_batch += a_batch_agent\n",
    "                r_batch += r_batch_agent\n",
    "                ad_batch += ad_batch_agent\n",
    "           \n",
    "            assert len(s_batch)*game.max_moves == len(a_batch)\n",
    "            #used shared RMSProp, i.e., shared g\n",
    "            actions = np.eye(game.action_dim, dtype=np.float32)[np.array(a_batch)]\n",
    "            entropy, gradients = network.policy_train(np.array(s_batch), \n",
    "                                                      actions, \n",
    "                                                      np.vstack(ad_batch).astype(np.float32), \n",
    "                                                      config.entropy_weight)\n",
    "\n",
    "            if GRADIENTS_CHECK:\n",
    "                for g in range(len(gradients)):\n",
    "                    assert np.any(np.isnan(gradients[g])) == False, (s_batch, a_batch, r_batch)\n",
    "            \n",
    "            if step % config.save_step == config.save_step - 1:\n",
    "                network.update_chkpt_saving_dir(config,commitment_window,look_ahead_window,step)\n",
    "                network.save_ckpt(_print=True)\n",
    "                \n",
    "                #log training information\n",
    "                learning_rate = network.lr_schedule(network.optimizer.iterations.numpy()).numpy()\n",
    "                avg_reward = np.mean(r_batch)\n",
    "                avg_advantage = np.mean(ad_batch)\n",
    "                avg_entropy = np.mean(entropy)\n",
    "                network.inject_summaries({\n",
    "                    'learning rate': learning_rate,\n",
    "                    'avg reward': avg_reward,\n",
    "                    'avg advantage': avg_advantage,\n",
    "                    'avg entropy': avg_entropy\n",
    "                    }, step)\n",
    "                print('lr:%f, avg reward:%f, avg advantage:%f, avg entropy:%f'%(learning_rate, avg_reward, avg_advantage, avg_entropy))\n",
    "\n",
    "def agent(agent_id, config, game, each_flow_paths,each_path_edges,each_flow_shortest_path,each_path_id,each_id_path,commitment_window,look_ahead_window,tm_subset, model_weights_queue, experience_queue):\n",
    "    random_state = np.random.RandomState(seed=agent_id)\n",
    "    network = Network(config, game.state_dims, game.action_dim, game.max_moves,commitment_window,look_ahead_window, 1, master=False)\n",
    "\n",
    "    # initial synchronization of the model weights from the coordinator \n",
    "    model_weights = model_weights_queue.get()\n",
    "    network.model.set_weights(model_weights)\n",
    "\n",
    "    idx = 0\n",
    "    s_batch = []\n",
    "    a_batch = []\n",
    "    r_batch = []\n",
    "    if config.method == 'pure_policy':\n",
    "        ad_batch = []\n",
    "    run_iteration_idx = 0\n",
    "    num_tms = len(tm_subset)\n",
    "    #print('this is random state before shuffling',random_state)\n",
    "    #random_state.shuffle(tm_subset)\n",
    "    #print('this is random state after shuffling',random_state)\n",
    "    run_iterations = FLAGS.num_iter\n",
    "    #print(\"****************--------------------------------------***************run_iterations\",run_iterations)\n",
    "    import pdb\n",
    "    #pdb.set_trace()\n",
    "    while True:\n",
    "        \n",
    "        tm_idx = tm_subset[idx]\n",
    "        #print(\"idx is %s and tm_idx is %s and num_tms is %s\"%(idx,tm_idx,num_tms))\n",
    "        #state\n",
    "        if tm_idx <=num_tms -(look_ahead_window+2):\n",
    "            state = game.get_state(tm_idx)\n",
    "            #print(state)\n",
    "            import pdb\n",
    "            #pdb.set_trace()\n",
    "            s_batch.append(state)\n",
    "            #action\n",
    "            if config.method == 'actor_critic':    \n",
    "                policy = network.actor_predict(np.expand_dims(state, 0)).numpy()[0]\n",
    "                #print(\"\\n \\n \\n\")\n",
    "                #print('here is the policy',policy)\n",
    "                import pdb\n",
    "                #pdb.set_trace()\n",
    "            elif config.method == 'pure_policy':\n",
    "                policy = network.policy_predict(np.expand_dims(state, 0)).numpy()[0]\n",
    "            assert np.count_nonzero(policy) >= game.max_moves, (policy, state)\n",
    "            actions = random_state.choice(game.action_dim, game.max_moves, p=policy, replace=False)\n",
    "            #print(len(policy))\n",
    "\n",
    "            #print('and here is the actions',game.action_dim,game.max_moves,actions)\n",
    "\n",
    "            #print(\"these must be the same as actions\",np.argpartition(policy, -4)[-4:])\n",
    "            import pdb\n",
    "            #pdb.set_trace()\n",
    "            for a in actions:\n",
    "                a_batch.append(a)\n",
    "\n",
    "            #reward\n",
    "            #reward = game.reward(tm_idx, actions)\n",
    "            \"\"\" safe online learning section\"\"\"\n",
    "\n",
    "            \"\"\"we check if for all the flows there is atleast one path in the chosen paths\n",
    "            if not, we will use the shortest path for that flow\"\"\"\n",
    "\n",
    "            #each_flow_shortest_path = env.get_each_flow_shortest_paths()\n",
    "#             for flow, shortest_path in each_flow_shortest_path.items():\n",
    "#                 print(\"for flow %s we have shortest path %s \"%(flow,shortest_path))\n",
    "            import pdb\n",
    "\n",
    "            #print(\"the rl has selected %s paths \"%(len(actions)))\n",
    "            for flow,paths in each_flow_paths.items():\n",
    "                covered_flow = False\n",
    "                for path in paths:\n",
    "                    \n",
    "                    if not covered_flow:\n",
    "                        if path in actions:\n",
    "                            covered_flow =True \n",
    "                if not covered_flow:\n",
    "#                     print(\"flow  did not have any candidate path in chosen paths\",flow)\n",
    "                    flow_shortest_path = each_flow_shortest_path[flow]\n",
    "#                     print(flow_shortest_path)\n",
    "                    #print(each_path_id)\n",
    "                    path_id = each_path_id[tuple(flow_shortest_path)]\n",
    "            \n",
    "                    actions = np.append(actions, path_id)\n",
    "#                     print(\"we added path id %s for safety for flow %s\"%(path_id,flow))\n",
    "\n",
    "            \"\"\"end of safe online learning section\"\"\"\n",
    "\n",
    "    #         for path_id in actions:\n",
    "    #             print(\"we have chosen and safed action %s %s\"%(path_id,len(actions)))\n",
    "\n",
    "            each_flow_selected_paths = {}\n",
    "            \"\"\"we now add the edges for each flow from the selected actions\"\"\"\n",
    "            each_flow_edges = {}\n",
    "            for flow in each_flow_shortest_path:\n",
    "                for path_id in actions:\n",
    "                    path = each_path_edges[path_id]\n",
    "                    if path_id in each_flow_paths[flow]:\n",
    "                        this_path_edges = each_path_edges[path_id]\n",
    "                        try:\n",
    "                            each_flow_selected_paths[flow].append(path_id)\n",
    "                        except:\n",
    "                            each_flow_selected_paths[flow]=[path_id]\n",
    "                        for edge in path:\n",
    "#                             print(\"we are adding edge \",edge)\n",
    "                            try:\n",
    "                                if edge not in each_flow_edges[flow] and (edge[1],edge[0]) not in each_flow_edges[flow]:\n",
    "                                    each_flow_edges[flow].append(edge)\n",
    "                                    each_flow_edges[flow].append((edge[1],edge[0]))\n",
    "                            except:\n",
    "                                each_flow_edges[flow]=[edge]\n",
    "                                each_flow_edges[flow].append((edge[1],edge[0]))\n",
    "                                \n",
    "                                \n",
    "#                 if flow ==(0, 5):\n",
    "#                     print(\"each_flow_paths\",each_flow_paths)\n",
    "#                     print(\"here are the edges for this flow \",each_flow_edges[flow])\n",
    "\n",
    "#             for flow,edges in each_flow_edges.items():\n",
    "#                 print(\"this flow %s has these edges %s\"%(flow,edges))\n",
    "            p_counter = 0\n",
    "            for flow,paths in each_flow_selected_paths.items():\n",
    "                for path in paths:\n",
    "                    p_counter+=1\n",
    "                    #print(\"flow %s uses path %s \"%(flow,path))\n",
    "            #print(\"we have chosen %s paths for % flows \"%(p_counter,len(list(each_flow_selected_paths.keys()))))\n",
    "            #pdb.set_trace()\n",
    "\n",
    "    #         for path_id in actions:\n",
    "    #             each_flow_edges[]\n",
    "            reward = game.reward2(tm_idx,num_tms,look_ahead_window,each_flow_edges,each_flow_paths,each_path_id,each_id_path,each_flow_shortest_path,config.topology_file,config.printing_flag)\n",
    "            r_batch.append(reward)\n",
    "\n",
    "            if config.method == 'pure_policy':\n",
    "                #advantage\n",
    "                if config.baseline == 'avg':\n",
    "                    ad_batch.append(game.advantage(tm_idx, reward))\n",
    "                    game.update_baseline(tm_idx, reward)\n",
    "                elif config.baseline == 'best':\n",
    "                    best_actions = policy.argsort()[-game.max_moves:]\n",
    "                    best_reward = game.reward(tm_idx, best_actions)\n",
    "                    ad_batch.append(reward - best_reward)\n",
    "\n",
    "            run_iteration_idx += 1\n",
    "            if run_iteration_idx >= run_iterations:\n",
    "                # Report experience to the coordinator                          \n",
    "                if config.method == 'actor_critic':    \n",
    "                    experience_queue.put([s_batch, a_batch, r_batch])\n",
    "                elif config.method == 'pure_policy':\n",
    "                    experience_queue.put([s_batch, a_batch, r_batch, ad_batch])\n",
    "\n",
    "                #print('report', agent_id)\n",
    "\n",
    "                # synchronize the network parameters from the coordinator\n",
    "                model_weights = model_weights_queue.get()\n",
    "                network.model.set_weights(model_weights)\n",
    "\n",
    "                del s_batch[:]\n",
    "                del a_batch[:]\n",
    "                del r_batch[:]\n",
    "                if config.method == 'pure_policy':\n",
    "                    del ad_batch[:]\n",
    "                run_iteration_idx = 0\n",
    "\n",
    "        # Update idx\n",
    "        idx += 1\n",
    "        if idx+game.look_ahead_window+2 >= num_tms:\n",
    "           random_state.shuffle(tm_subset)\n",
    "           idx = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c8992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    #cpu only\n",
    "    tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "    tf.get_logger().setLevel('INFO')\n",
    "    #tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "    config = get_config(FLAGS) or FLAGS\n",
    "    \n",
    "    \"\"\"we get the number of possible paths in the network, \n",
    "    avg paths used by all flows over times and each flows all used paths\"\"\"\n",
    "    path_counter,avg_paths_per_time,each_flow_paths= extract_paths(config.topology_file,config.each_topology_each_t_each_f_paths)\n",
    "    each_path_id = {}\n",
    "    id_counter = 0\n",
    "    each_id_path = {}\n",
    "    covered_paths = []\n",
    "    each_path_edges = {}\n",
    "    new_each_flow_paths = {}\n",
    "    for flow,paths in each_flow_paths.items():\n",
    "        for p in paths:\n",
    "            covered_paths.append(p)\n",
    "            each_path_id[tuple(p)] = id_counter\n",
    "            try:\n",
    "                new_each_flow_paths[flow].append(id_counter)\n",
    "            except:\n",
    "                new_each_flow_paths[flow] = [id_counter]\n",
    "            each_id_path[id_counter] = tuple(p)\n",
    "            path = tuple(p)\n",
    "            for node_indx in range(len(path)-1):\n",
    "                try:\n",
    "                    if (path[node_indx],path[node_indx+1]) not in each_path_edges[id_counter]:\n",
    "                        each_path_edges[id_counter].append((path[node_indx],path[node_indx+1]))\n",
    "                        each_path_edges[id_counter].append((path[node_indx+1],path[node_indx]))\n",
    "                except:\n",
    "                    each_path_edges[id_counter]=[(path[node_indx],path[node_indx+1])]\n",
    "                    each_path_edges[id_counter].append((path[node_indx+1],path[node_indx]))\n",
    "            \n",
    "            id_counter+=1\n",
    "            \n",
    "    env = Environment(config,is_training=True)\n",
    "    #print(\"env.num_nodes\",env.num_nodes)\n",
    "    import pdb\n",
    "    #pdb.set_trace()\n",
    "    each_flow_shortest_paths = env.topology.get_each_flow_shortest_paths()\n",
    "    for flow,p in each_flow_shortest_paths.items():\n",
    "        p = tuple(p)\n",
    "        if p not in covered_paths:\n",
    "            each_path_id[tuple(p)] = id_counter\n",
    "            try:\n",
    "                new_each_flow_paths[flow].append(id_counter)\n",
    "            except:\n",
    "                new_each_flow_paths[flow] = [id_counter]\n",
    "                \n",
    "            each_id_path[id_counter] = tuple(p)\n",
    "            path = tuple(p)\n",
    "            for node_indx in range(len(path)-1):\n",
    "                try:\n",
    "                    if (path[node_indx],path[node_indx+1]) not in each_path_edges[id_counter]:\n",
    "                        each_path_edges[id_counter].append((path[node_indx],path[node_indx+1]))\n",
    "                        each_path_edges[id_counter].append((path[node_indx+1],path[node_indx]))\n",
    "                except:\n",
    "                    each_path_edges[id_counter]=[(path[node_indx],path[node_indx+1])]\n",
    "                    each_path_edges[id_counter].append((path[node_indx+1],path[node_indx]))\n",
    "            id_counter+=1\n",
    "            \n",
    "    for commitment_window in [10]:\n",
    "        #for look_ahead_window in range(4,int(config.look_ahead_window_range)):\n",
    "        for look_ahead_window in [4]:\n",
    "            \"\"\"we first find the candidate paths and use it for action dimention\"\"\"\n",
    "            \n",
    "            \n",
    "            game = CFRRL_Game(config, env,commitment_window,look_ahead_window,path_counter,avg_paths_per_time)\n",
    "            \"\"\"we modify and set the max move to the avg used path at each t for  all flows\"\"\"\n",
    "            game.max_moves = avg_paths_per_time\n",
    "            #print('this would be the action dimention',path_counter)\n",
    "            import pdb\n",
    "            #pdb.set_trace()\n",
    "            model_weights_queues = []\n",
    "            experience_queues = []\n",
    "            if FLAGS.num_agents == 0 or FLAGS.num_agents >= mp.cpu_count():\n",
    "                FLAGS.num_agents = mp.cpu_count() - 1\n",
    "            #print('Agent num: %d, iter num: %d\\n'%(FLAGS.num_agents+1, FLAGS.num_iter))\n",
    "            for _ in range(FLAGS.num_agents):\n",
    "#                 print(\"we are ddding one to weight queue \")\n",
    "                model_weights_queues.append(mp.Queue(1))\n",
    "                experience_queues.append(mp.Queue(1))\n",
    "                import pdb\n",
    "            #pdb.set_trace()\n",
    "            tm_subsets = np.array_split(game.tm_indexes, FLAGS.num_agents)\n",
    "\n",
    "            coordinator = mp.Process(target=central_agent, args=(config, game,commitment_window,look_ahead_window, model_weights_queues, experience_queues))\n",
    "\n",
    "            coordinator.start()\n",
    "\n",
    "            agents = []\n",
    "            print(\"these are the agents \",FLAGS.num_agents)\n",
    "            import pdb\n",
    "            #pdb.set_trace()\n",
    "            for i in range(FLAGS.num_agents):\n",
    "                agents.append(mp.Process(target=agent, args=(i, config, game,new_each_flow_paths,each_path_edges,each_flow_shortest_paths,each_path_id,each_id_path,commitment_window, look_ahead_window,tm_subsets[i], model_weights_queues[i], experience_queues[i])))\n",
    "\n",
    "            for i in range(FLAGS.num_agents):\n",
    "                agents[i].start()\n",
    "\n",
    "            coordinator.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7663d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(main)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
